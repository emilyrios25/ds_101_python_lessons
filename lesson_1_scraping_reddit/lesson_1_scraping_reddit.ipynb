{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2c03e9",
   "metadata": {},
   "source": [
    "# Lesson 1:  Reddit Comment Scraper\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Extract comments** from any Reddit thread\n",
    "2. **Change the target URL** to scrape different discussions\n",
    "4. **Change extraction quantity** to scrape more or less data\n",
    "3. **Save data to CSV** for further analysis\n",
    "\n",
    "\n",
    "## üöÄ What You'll Build\n",
    "A simple but powerful Reddit comment scraper that can:\n",
    "- Take any Reddit thread URL\n",
    "- Extract all comments with metadata (author, score, text, timestamp)\n",
    "- Save the results to a CSV file for use in other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8434bc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "üì¶ Ready to scrape Reddit comments\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üì¶ Ready to scrape Reddit comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d4662",
   "metadata": {},
   "source": [
    "## Reddit API Authentication\n",
    "\n",
    "### Overview\n",
    "Reddit allows users to \"scrape\" data from their website using an API (Application Programming Interface). Because \"scraping\" can be taxing on their servers, you have to authenticate as a user to scrape more posts per minute. The script below authenticates you as a user so you can \"scrape\" (download posts) at a higher limit. If you are not authenticated, you can still get data, but it's slower.\n",
    "\n",
    "### üéØ Two Authentication Methods:\n",
    "\n",
    "| Method | Rate Limit | Best For |\n",
    "|--------|------------|----------|\n",
    "| **Authenticated (Encrypted)** | 600 requests/minute | Large subreddits, many posts |\n",
    "| **Anonymous (Read-only)** | 60 requests/minute | Single threads, small datasets |\n",
    "\n",
    "### üîí Security:\n",
    "I have set up **encrypted credentials** that give you higher rate limits while keeping the actual login details secure. All of this authentication code has been hidden from you in a separate module.\n",
    "\n",
    "### üîß Setup:\n",
    "The authentication cell below will **automatically** choose the best available method - no setup needed on your end! All the complex authentication code is hidden in a separate file to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7e4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connecting to Reddit...\n",
      "‚úÖ Read-only connection ready! (60 requests/minute)\n",
      "üéØ Ready to scrape!\n",
      "‚úÖ Read-only connection ready! (60 requests/minute)\n",
      "üéØ Ready to scrape!\n"
     ]
    }
   ],
   "source": [
    "# üîß Setup Reddit Connection\n",
    "\n",
    "\n",
    "# Clear any previous imports to avoid caching issues\n",
    "import importlib\n",
    "import sys\n",
    "if 'reddit_auth' in sys.modules:\n",
    "    importlib.reload(sys.modules['reddit_auth'])\n",
    "\n",
    "from reddit_auth import setup_reddit_connection\n",
    "\n",
    "print(\"üîó Connecting to Reddit...\")\n",
    "\n",
    "# Single function call with explicit variable assignment\n",
    "reddit, auth_mode, rate_limit = setup_reddit_connection()\n",
    "\n",
    "# Single, clear status message\n",
    "status_msg = \"‚úÖ Authenticated connection ready! (600 requests/minute)\" if auth_mode == \"authenticated\" else \"‚úÖ Read-only connection ready! (60 requests/minute)\"\n",
    "print(status_msg)\n",
    "print(\"üéØ Ready to scrape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1232a9f",
   "metadata": {},
   "source": [
    "## STEP 1: Choose Your Reddit Thread\n",
    "\n",
    "To find a thread URL:\n",
    "1. Go to Reddit.com\n",
    "2. Find an interesting post with lots of comments\n",
    "3. Copy the full URL from your browser\n",
    "4. Paste it below (replace the current URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e3f3fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Target URL: https://www.reddit.com/r/jmu/comments/1lbrjnx/best_jmu_suitestyle_halls_need_help_ranking_area/\n",
      "üìù To change this, modify the 'url' variable above\n",
      "‚ùå Error loading thread: name 'reddit' is not defined\n",
      "üí° Make sure the URL is a valid Reddit thread URL\n"
     ]
    }
   ],
   "source": [
    "#Change this URL\n",
    "url = \"https://www.reddit.com/r/jmu/comments/1lbrjnx/best_jmu_suitestyle_halls_need_help_ranking_area/\"\n",
    "\n",
    "print(f\"üîó Target URL: {url}\")\n",
    "print(\"üìù To change this, modify the 'url' variable above\")\n",
    "\n",
    "# Load the Reddit thread\n",
    "try:\n",
    "    submission = reddit.submission(url=url)\n",
    "    print(f\"\\n‚úÖ Successfully loaded thread!\")\n",
    "    print(f\"üì∞ Title: '{submission.title}'\")\n",
    "    print(f\"üìä Score: {submission.score}\")\n",
    "    print(f\"üí¨ Comments: {submission.num_comments}\")\n",
    "    print(f\"üìÖ Subreddit: r/{submission.subreddit}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading thread: {e}\")\n",
    "    print(\"üí° Make sure the URL is a valid Reddit thread URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914c795",
   "metadata": {},
   "source": [
    "## STEP 2: Extract All Comments from the Thread\n",
    "\n",
    "Once you have downloaded all of the comments, these are bundled in a special format. The comments need to be \"extracted\" into something humans can read. The function below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045ad06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting comments from the thread...\n",
      "‚è≥ This may take a few seconds for threads with many comments...\n",
      "‚úÖ Successfully extracted 5 comments!\n",
      "\n",
      "üìã Preview of first 3 comments:\n",
      "\n",
      "1. Author: Pitiful-Pickle-5101 | Score: 2\n",
      "   Text: Do you mean Jack and Jill style bathrooms? Or like village suite style where 3 sets of roommates sha...\n",
      "   Date: 2025-06-15 14:56:40\n",
      "\n",
      "2. Author: An51759 | Score: 1\n",
      "   Text: Also I heard assignment is random, I was wondering how I‚Äôll be able to pick a suite?...\n",
      "   Date: 2025-06-15 13:13:36\n",
      "\n",
      "3. Author: flutiful_fiona | Score: 1\n",
      "   Text: the village dorms are suite style, but don't live there unless you like to party. the bathrooms get ...\n",
      "   Date: 2025-06-20 13:02:33\n"
     ]
    }
   ],
   "source": [
    "# üîç STEP 2: Extract All Comments from the Thread\n",
    "\n",
    "print(\"üîç Extracting comments from the thread...\")\n",
    "print(\"‚è≥ This may take a few seconds for threads with many comments...\")\n",
    "\n",
    "comments_data = []\n",
    "\n",
    "try:\n",
    "    # Loop through all top-level comments\n",
    "    for comment in submission.comments:\n",
    "        if hasattr(comment, 'body'):  # Skip \"MoreComments\" objects\n",
    "            # Convert timestamp to readable date\n",
    "            comment_date = datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Store comment information\n",
    "            comment_info = {\n",
    "                'author': str(comment.author) if comment.author else '[deleted]',\n",
    "                'score': comment.score,\n",
    "                'text': comment.body,\n",
    "                'date': comment_date,\n",
    "                'thread_title': submission.title,\n",
    "                'subreddit': str(submission.subreddit)\n",
    "            }\n",
    "            comments_data.append(comment_info)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully extracted {len(comments_data)} comments!\")\n",
    "    \n",
    "    # Show a preview of the first few comments\n",
    "    if comments_data:\n",
    "        print(f\"\\nüìã Preview of first 3 comments:\")\n",
    "        for i, comment in enumerate(comments_data[:3], 1):\n",
    "            print(f\"\\n{i}. Author: {comment['author']} | Score: {comment['score']}\")\n",
    "            print(f\"   Text: {comment['text'][:100]}...\")\n",
    "            print(f\"   Date: {comment['date']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error extracting comments: {e}\")\n",
    "    comments_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb96ed6",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Note that we now get a visual of the Author, Score, Text, and Date of each comment. The main issue is that we are going comment-by-comment, which are unlikely to have a lot of data in them, unless it's a hot topic. Instead, we want to grab the entire JMU subreddit. This is slightly more complicated, because we want each post and then the subsequent comments. It also means we will get A LOT of data. We want to be able to limit this somehow. Likewise, we don't necessarily want to grab just a random sample but grab things that seem the most relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12076d7c",
   "metadata": {},
   "source": [
    "# \udcbe STEP 3: Save Your Data to CSV\n",
    "\n",
    "Now let's save the scraped comments to a CSV file that you can open in Excel, Google Sheets, or use for further analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c167ba6",
   "metadata": {},
   "source": [
    "# üéØ Try Different Threads!\n",
    "\n",
    "Now that you have a working scraper, try it with different Reddit threads:\n",
    "\n",
    "## üî• Suggested Thread Types:\n",
    "\n",
    "### Current Events & News\n",
    "- r/worldnews - Global news discussions\n",
    "- r/politics - Political discussions\n",
    "- r/technology - Tech news and discussions\n",
    "\n",
    "### Questions & Discussions  \n",
    "- r/AskReddit - Open-ended questions\n",
    "- r/explainlikeimfive - Simple explanations\n",
    "- r/changemyview - Debate and discussion\n",
    "\n",
    "### Hobbies & Interests\n",
    "- r/movies - Film discussions\n",
    "- r/gaming - Video game discussions  \n",
    "- r/science - Scientific discussions\n",
    "\n",
    "## üìù How to Use:\n",
    "1. **Find a thread**: Browse Reddit and find an interesting post with lots of comments\n",
    "2. **Copy the URL**: Copy the full URL from your browser address bar\n",
    "3. **Update the code**: Change the `url` variable in Step 1\n",
    "4. **Run again**: Execute the cells to scrape the new thread\n",
    "5. **Save**: Each run creates a new CSV file with timestamp\n",
    "\n",
    "## üí° Pro Tips:\n",
    "- **Popular threads** have more comments but take longer to scrape\n",
    "- **Recent threads** may have more active discussions\n",
    "- **Different subreddits** have different discussion styles and topics\n",
    "- **Sort by \"Hot\" or \"Top\"** to find the most engaging threads\n",
    "\n",
    "## üìä What's in Your CSV File:\n",
    "- **author**: Username who posted the comment\n",
    "- **score**: Upvotes minus downvotes  \n",
    "- **text**: The actual comment text\n",
    "- **date**: When the comment was posted\n",
    "- **thread_title**: Title of the Reddit thread\n",
    "- **subreddit**: Which subreddit the thread is from\n",
    "\n",
    "## \ude80 Next Steps:\n",
    "- Try analyzing your CSV data in Excel or Google Sheets\n",
    "- Look for patterns in comment scores or lengths\n",
    "- Compare discussions across different subreddits\n",
    "- Use the data for sentiment analysis or word cloud generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb615107",
   "metadata": {},
   "source": [
    "# Step 3: Scraping Entire Subreddits\n",
    "\n",
    "The scraper below takes three main variables: \n",
    "- `subreddit_name` - the name of the subreddit without /r\n",
    "- `num_posts` - The number of posts you want to scrape (note that you are limited to 600 posts/minute)\n",
    "- `sort_method` - Options: \"hot\", \"new\", \"top\", \"rising\". Reddit uses these to organize posts\n",
    "\n",
    "You can experiment with the different settings to get a collection of posts that will show up in the output. Keep in mind, how you extract the data determines what data you'll be analyzing. If you are only looking at \"new\" comments, you might miss a major issue in the community. If you only look at \"top\" comments, you'll miss what folks are currently concerned with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79b754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Scraping r/JMU for text analysis\n",
      "üìä Getting 10 top posts...\n",
      "‚è≥ This may take a minute...\n",
      "üìù Processing post 1/10: President Alger leaving to take same job at Americ...\n",
      "üìù Processing post 2/10: Alger‚Äôs response to hitting over 500 cases in a we...\n",
      "üìù Processing post 3/10: Virginia schools be like...\n",
      "üìù Processing post 2/10: Alger‚Äôs response to hitting over 500 cases in a we...\n",
      "üìù Processing post 3/10: Virginia schools be like...\n",
      "üìù Processing post 4/10: The Dukes Advance!...\n",
      "üìù Processing post 5/10: took My graduation pictures today while maintainin...\n",
      "üìù Processing post 4/10: The Dukes Advance!...\n",
      "üìù Processing post 5/10: took My graduation pictures today while maintainin...\n",
      "üìù Processing post 6/10: Taking Senior Photos During JMU Construction (2019...\n",
      "üìù Processing post 7/10: Campus Reopening Plan...\n",
      "üìù Processing post 6/10: Taking Senior Photos During JMU Construction (2019...\n",
      "üìù Processing post 7/10: Campus Reopening Plan...\n",
      "üìù Processing post 8/10: Leaving Hburg today, and this‚Äôll be my last impres...\n",
      "üìù Processing post 9/10: JMU images featured in Daily Show‚Äôs ‚ÄúPandumbic‚Äù co...\n",
      "üìù Processing post 8/10: Leaving Hburg today, and this‚Äôll be my last impres...\n",
      "üìù Processing post 9/10: JMU images featured in Daily Show‚Äôs ‚ÄúPandumbic‚Äù co...\n",
      "üìù Processing post 10/10: Truly a revolutionary concept...\n",
      "\n",
      "‚úÖ Successfully collected 130 text items!\n",
      "üìä Ready for text analysis from r/JMU\n",
      "üìù Processing post 10/10: Truly a revolutionary concept...\n",
      "\n",
      "‚úÖ Successfully collected 130 text items!\n",
      "üìä Ready for text analysis from r/JMU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subreddit_name = \"JMU\"  # Change this to any subreddit (without r/)\n",
    "num_posts = 10         # How many posts to scrape\n",
    "sort_method = \"top\"     # Options: \"hot\", \"new\", \"top\", \"rising\"\n",
    "\n",
    "print(f\"üéØ Scraping r/{subreddit_name} for text analysis\")\n",
    "print(f\"üìä Getting {num_posts} {sort_method} posts...\")\n",
    "print(\"‚è≥ This may take a minute...\")\n",
    "\n",
    "# Simple data structure - no duplicates\n",
    "text_data = []\n",
    "\n",
    "try:\n",
    "    # Get the subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Choose sorting method\n",
    "    if sort_method == \"hot\":\n",
    "        posts = subreddit.hot(limit=num_posts)\n",
    "    elif sort_method == \"new\":\n",
    "        posts = subreddit.new(limit=num_posts)\n",
    "    elif sort_method == \"top\":\n",
    "        posts = subreddit.top(limit=num_posts)\n",
    "    elif sort_method == \"rising\":\n",
    "        posts = subreddit.rising(limit=num_posts)\n",
    "    else:\n",
    "        posts = subreddit.hot(limit=num_posts)\n",
    "    \n",
    "    # Loop through each post\n",
    "    for post_num, submission in enumerate(posts, 1):\n",
    "        print(f\"üìù Processing post {post_num}/{num_posts}: {submission.title[:50]}...\")\n",
    "        \n",
    "        # Get post date for reference\n",
    "        post_date = datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Add the post itself (title + content)\n",
    "        post_text = submission.title\n",
    "        if submission.selftext.strip():  # Add post content if it exists\n",
    "            post_text += \" \" + submission.selftext\n",
    "            \n",
    "        text_data.append({\n",
    "            'type': 'post',\n",
    "            'title': submission.title,\n",
    "            'text': post_text,\n",
    "            'date': post_date,\n",
    "            'score': submission.score\n",
    "        })\n",
    "        \n",
    "        # Get comments for this post\n",
    "        try:\n",
    "            submission.comments.replace_more(limit=0)  # Don't expand \"more comments\"\n",
    "            \n",
    "            for comment in submission.comments.list()[:50]:  # Get more comments per post\n",
    "                if hasattr(comment, 'body') and comment.body.strip():\n",
    "                    comment_date = datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    \n",
    "                    text_data.append({\n",
    "                        'type': 'comment',\n",
    "                        'title': submission.title,  # Thread title for reference\n",
    "                        'text': comment.body,\n",
    "                        'date': comment_date,\n",
    "                        'score': comment.score\n",
    "                    })\n",
    "        except:\n",
    "            print(f\"   ‚ö†Ô∏è Could not load comments for this post\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully collected {len(text_data)} text items!\")\n",
    "    print(f\"üìä Ready for text analysis from r/{subreddit_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error scraping subreddit: {e}\")\n",
    "    print(\"üí° Make sure the subreddit name is correct and accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab304ac",
   "metadata": {},
   "source": [
    "# Step 4: Generate Output\n",
    "\n",
    "Once the data has been created, you want to write that to a file. The script below creates two different files:\n",
    "\n",
    "- `reddit_text_analysis...csv`\n",
    "- `reddit_voyant.txt`\n",
    "\n",
    "We will use both files eventually. The `.csv` file is a table of all the comments and posts and the `.txt` file is the same information, but only as text, which is useful for visualizing in Voyant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18831ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà TEXT ANALYSIS STATS:\n",
      "   ‚Ä¢ Posts: 10\n",
      "   ‚Ä¢ Comments: 112\n",
      "   ‚Ä¢ Total words: 2,817\n",
      "   ‚Ä¢ Average text length: 133 characters\n",
      "   ‚Ä¢ Longest text: 857 characters\n",
      "\n",
      "üìã SAMPLE TEXTS:\n",
      "1. [POST] President Alger leaving to take same job at American University at the end of this academic year...\n",
      "2. [COMMENT] Like him or not, he did help transform this school. Applications to JMU have drastically increased u...\n",
      "3. [COMMENT] Massive changes happening at JMU this year. Alger stepping down, AD Bourne retiring, Cignetti left f...\n",
      "\n",
      "üéØ Created Voyant-ready file: data\\reddit_voyant_JMU_20250905_041030.txt\n",
      "\n",
      "üí° TO RELOAD DATA WITH PROPER TYPES:\n",
      "   CSV method:\n",
      "   df = pd.read_csv('data\\reddit_text_analysis_JMU_top_num_posts_10_20250905_041030.csv')\n",
      "   df['score'] = pd.to_numeric(df['score'])\n",
      "   df['date'] = pd.to_datetime(df['date'])\n",
      "   df['type'] = df['type'].astype('category')\n",
      "   \n",
      "   Pickle method (easiest):\n",
      "   df = pd.read_pickle('data\\reddit_data_JMU_20250905_040840.pkl')\n"
     ]
    }
   ],
   "source": [
    "# üíæ Save Clean Text Data for Topic Modeling\n",
    "\n",
    "if text_data:\n",
    "    try:\n",
    "        # Create data folder if it doesn't exist\n",
    "        data_folder = \"data\"\n",
    "        os.makedirs(data_folder, exist_ok=True)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df_text = pd.DataFrame(text_data)\n",
    "        \n",
    "        # Clean text function for analysis\n",
    "        def clean_text_for_analysis(text):\n",
    "            if pd.isna(text):\n",
    "                return \"\"\n",
    "            text = str(text)\n",
    "            # Handle encoding\n",
    "            text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            # Remove control characters but keep newlines\n",
    "            text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\r\\t')\n",
    "            # Remove extra whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            return text\n",
    "        \n",
    "        # Clean the text data\n",
    "        df_text['text'] = df_text['text'].apply(clean_text_for_analysis)\n",
    "        df_text['title'] = df_text['title'].apply(clean_text_for_analysis)\n",
    "        \n",
    "        # Remove empty entries\n",
    "        df_text = df_text[df_text['text'].str.len() > 10]  # Remove very short texts\n",
    "        \n",
    "        # Ensure proper data types\n",
    "        df_text['score'] = pd.to_numeric(df_text['score'], errors='coerce')\n",
    "        df_text['date'] = pd.to_datetime(df_text['date'], errors='coerce')\n",
    "        df_text['type'] = df_text['type'].astype('category')\n",
    "        \n",
    "        # Create filename for analysis (save to data folder)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = os.path.join(data_folder, f\"reddit_text_analysis_{subreddit_name}_{sort_method}_num_posts_{num_posts}_{timestamp}.csv\")\n",
    "        \n",
    "        # Save to CSV optimized for text analysis\n",
    "        df_text.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "     \n",
    "        # Show statistics for text analysis\n",
    "        posts_count = (df_text['type'] == 'post').sum()\n",
    "        comments_count = (df_text['type'] == 'comment').sum()\n",
    "        \n",
    "        print(f\"\\nüìà TEXT ANALYSIS STATS:\")\n",
    "        print(f\"   ‚Ä¢ Posts: {posts_count}\")\n",
    "        print(f\"   ‚Ä¢ Comments: {comments_count}\")\n",
    "        print(f\"   ‚Ä¢ Total words: {df_text['text'].str.split().str.len().sum():,}\")\n",
    "        print(f\"   ‚Ä¢ Average text length: {df_text['text'].str.len().mean():.0f} characters\")\n",
    "        print(f\"   ‚Ä¢ Longest text: {df_text['text'].str.len().max()} characters\")\n",
    "        \n",
    "        print(f\"\\nüìã SAMPLE TEXTS:\")\n",
    "        for idx, row in enumerate(df_text.head(3).iterrows(), 1):\n",
    "            _, row_data = row\n",
    "            print(f\"{idx}. [{row_data['type'].upper()}] {row_data['text'][:100]}...\")\n",
    "        \n",
    "        # Create a separate file with just the text for Voyant (save to data folder)\n",
    "        text_only_filename = os.path.join(data_folder, f\"reddit_voyant_{subreddit_name}_{timestamp}.txt\")\n",
    "        with open(text_only_filename, 'w', encoding='utf-8') as f:\n",
    "            for _, row in df_text.iterrows():\n",
    "                f.write(f\"{row['text']}\\n\\n\")\n",
    "        \n",
    "        print(f\"\\nüéØ Created Voyant-ready file: {text_only_filename}\")\n",
    "        \n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving text data: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No text data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f8e15",
   "metadata": {},
   "source": [
    "## üéØ Simplified Text Scraper for Topic Modeling\n",
    "\n",
    "### üìù What This Does:\n",
    "This streamlined version focuses on **clean text extraction** for analysis tools like Voyant:\n",
    "- **No duplicates**: Each piece of text appears only once\n",
    "- **Clean format**: Text optimized for analysis\n",
    "- **Two outputs**: CSV for data analysis + TXT file for Voyant\n",
    "\n",
    "### \udd27 Quick Setup:\n",
    "1. **Change subreddit**: `subreddit_name = \"JMU\"`\n",
    "2. **Set post count**: `num_posts = 100` \n",
    "3. **Choose sorting**: `sort_method = \"hot\"`\n",
    "4. **Run both cells above**\n",
    "\n",
    "### \udcca What You Get:\n",
    "\n",
    "#### CSV File Contains:\n",
    "- **type**: \"post\" or \"comment\"\n",
    "- **title**: Thread title (for context)\n",
    "- **text**: The actual text content\n",
    "- **date**: When it was posted\n",
    "- **score**: Reddit score\n",
    "\n",
    "#### TXT File Contains:\n",
    "- Pure text, one item per line\n",
    "- Perfect for uploading directly to Voyant\n",
    "- No metadata, just content\n",
    "\n",
    "### üéØ Perfect for Topic Modeling:\n",
    "- **Voyant Tools**: Upload the .txt file directly\n",
    "- **Other text analysis**: Use the .csv file\n",
    "- **Clean data**: Removed duplicates and empty entries\n",
    "- **Focused content**: Just the text you need\n",
    "\n",
    "### üí° Analysis Tips:\n",
    "- **Voyant**: Use the .txt file for word clouds, trends, correlations\n",
    "- **Text length**: Longer texts work better for topic modeling\n",
    "- **Sample size**: 50-100 posts usually gives good results\n",
    "- **Time periods**: Try different sorting methods to see trends\n",
    "\n",
    "This approach gives you exactly what you need for text analysis without the complexity!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
