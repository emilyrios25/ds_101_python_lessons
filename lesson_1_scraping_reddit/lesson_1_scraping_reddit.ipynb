{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2c03e9",
   "metadata": {},
   "source": [
    "# Lesson 1:  Reddit Comment Scraper\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Extract comments** from any Reddit thread\n",
    "2. **Change the target URL** to scrape different discussions\n",
    "4. **Change extraction quantity** to scrape more or less data\n",
    "3. **Save data to CSV** for further analysis\n",
    "\n",
    "\n",
    "## 🚀 What You'll Build\n",
    "A simple but powerful Reddit comment scraper that can:\n",
    "- Take any Reddit thread URL\n",
    "- Extract all comments with metadata (author, score, text, timestamp)\n",
    "- Save the results to a CSV file for use in other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8434bc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n",
      "📦 Ready to scrape Reddit comments\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(\"📦 Ready to scrape Reddit comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d4662",
   "metadata": {},
   "source": [
    "## Reddit API Authentication\n",
    "\n",
    "### Overview\n",
    "Reddit allows users to \"scrape\" data from their website using an API (Application Programming Interface). Because \"scraping\" can be taxing on their servers, you have to authenticate as a user to scrape more posts per minute. The script below authenticates you as a user so you can \"scrape\" (download posts) at a higher limit. If you are not authenticated, you can still get data, but it's slower.\n",
    "\n",
    "### 🎯 Two Authentication Methods:\n",
    "\n",
    "| Method | Rate Limit | Best For |\n",
    "|--------|------------|----------|\n",
    "| **Authenticated (Encrypted)** | 600 requests/minute | Large subreddits, many posts |\n",
    "| **Anonymous (Read-only)** | 60 requests/minute | Single threads, small datasets |\n",
    "\n",
    "### 🔒 Security:\n",
    "I have set up **encrypted credentials** that give you higher rate limits while keeping the actual login details secure. All of this authentication code has been hidden from you in a separate module.\n",
    "\n",
    "### 🔧 Setup:\n",
    "The authentication cell below will **automatically** choose the best available method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7e4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Connecting to Reddit...\n",
      "✅ Read-only connection ready! (60 requests/minute)\n",
      "🎯 Ready to scrape!\n",
      "✅ Read-only connection ready! (60 requests/minute)\n",
      "🎯 Ready to scrape!\n"
     ]
    }
   ],
   "source": [
    "# 🔧 Setup Reddit Connection\n",
    "\n",
    "# Clear any previous imports to avoid caching issues\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Add the config directory to the Python path\n",
    "import os\n",
    "config_path = os.path.join(os.getcwd(), 'config')\n",
    "if config_path not in sys.path:\n",
    "    sys.path.insert(0, config_path)\n",
    "\n",
    "# Clear cached modules\n",
    "if 'reddit_auth' in sys.modules:\n",
    "    importlib.reload(sys.modules['reddit_auth'])\n",
    "\n",
    "from reddit_auth import setup_reddit_connection\n",
    "\n",
    "print(\"🔗 Connecting to Reddit...\")\n",
    "\n",
    "# Single function call with explicit variable assignment\n",
    "reddit, auth_mode, rate_limit = setup_reddit_connection()\n",
    "\n",
    "# Single, clear status message\n",
    "status_msg = \"✅ Authenticated connection ready! (600 requests/minute)\" if auth_mode == \"authenticated\" else \"✅ Read-only connection ready! (60 requests/minute)\"\n",
    "print(status_msg)\n",
    "print(\"🎯 Ready to scrape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1232a9f",
   "metadata": {},
   "source": [
    "## STEP 1: Choose Your Reddit Thread\n",
    "\n",
    "To find a thread URL:\n",
    "1. Go to Reddit.com\n",
    "2. Find an interesting post with lots of comments\n",
    "3. Copy the full URL from your browser\n",
    "4. Paste it below (replace the current URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3f3fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 Target URL: https://www.reddit.com/r/jmu/comments/1lbrjnx/best_jmu_suitestyle_halls_need_help_ranking_area/\n",
      "📝 To change this, modify the 'url' variable above\n",
      "\n",
      "✅ Successfully loaded thread!\n",
      "📰 Title: 'Best JMU suite-style halls. Need help ranking area preferences (1-8) for housing'\n",
      "📊 Score: 4\n",
      "💬 Comments: 14\n",
      "📅 Subreddit: r/jmu\n"
     ]
    }
   ],
   "source": [
    "#Change this URL\n",
    "url = \"https://www.reddit.com/r/jmu/comments/1lbrjnx/best_jmu_suitestyle_halls_need_help_ranking_area/\"\n",
    "\n",
    "print(f\"🔗 Target URL: {url}\")\n",
    "print(\"📝 To change this, modify the 'url' variable above\")\n",
    "\n",
    "# Load the Reddit thread\n",
    "try:\n",
    "    submission = reddit.submission(url=url)\n",
    "    print(f\"\\n✅ Successfully loaded thread!\")\n",
    "    print(f\"📰 Title: '{submission.title}'\")\n",
    "    print(f\"📊 Score: {submission.score}\")\n",
    "    print(f\"💬 Comments: {submission.num_comments}\")\n",
    "    print(f\"📅 Subreddit: r/{submission.subreddit}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading thread: {e}\")\n",
    "    print(\"💡 Make sure the URL is a valid Reddit thread URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914c795",
   "metadata": {},
   "source": [
    "## STEP 2: Extract All Comments from the Thread\n",
    "\n",
    "Once you have downloaded all of the comments, these are bundled in a special format. The comments need to be \"extracted\" into something humans can read. The function below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045ad06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Extracting comments from the thread...\n",
      "⏳ This may take a few seconds for threads with many comments...\n",
      "✅ Successfully extracted 5 comments!\n",
      "\n",
      "📋 Preview of first 3 comments:\n",
      "\n",
      "1. Author: Pitiful-Pickle-5101 | Score: 2\n",
      "   Text: Do you mean Jack and Jill style bathrooms? Or like village suite style where 3 sets of roommates sha...\n",
      "   Date: 2025-06-15 14:56:40\n",
      "\n",
      "2. Author: An51759 | Score: 1\n",
      "   Text: Also I heard assignment is random, I was wondering how I’ll be able to pick a suite?...\n",
      "   Date: 2025-06-15 13:13:36\n",
      "\n",
      "3. Author: flutiful_fiona | Score: 1\n",
      "   Text: the village dorms are suite style, but don't live there unless you like to party. the bathrooms get ...\n",
      "   Date: 2025-06-20 13:02:33\n"
     ]
    }
   ],
   "source": [
    "# 🔍 STEP 2: Extract All Comments from the Thread\n",
    "\n",
    "print(\"🔍 Extracting comments from the thread...\")\n",
    "print(\"⏳ This may take a few seconds for threads with many comments...\")\n",
    "\n",
    "comments_data = []\n",
    "\n",
    "try:\n",
    "    # Loop through all top-level comments\n",
    "    for comment in submission.comments:\n",
    "        if hasattr(comment, 'body'):  # Skip \"MoreComments\" objects\n",
    "            # Convert timestamp to readable date\n",
    "            comment_date = datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Store comment information\n",
    "            comment_info = {\n",
    "                'author': str(comment.author) if comment.author else '[deleted]',\n",
    "                'score': comment.score,\n",
    "                'text': comment.body,\n",
    "                'date': comment_date,\n",
    "                'thread_title': submission.title,\n",
    "                'subreddit': str(submission.subreddit)\n",
    "            }\n",
    "            comments_data.append(comment_info)\n",
    "    \n",
    "    print(f\"✅ Successfully extracted {len(comments_data)} comments!\")\n",
    "    \n",
    "    # Show a preview of the first few comments\n",
    "    if comments_data:\n",
    "        print(f\"\\n📋 Preview of first 3 comments:\")\n",
    "        for i, comment in enumerate(comments_data[:3], 1):\n",
    "            print(f\"\\n{i}. Author: {comment['author']} | Score: {comment['score']}\")\n",
    "            print(f\"   Text: {comment['text'][:100]}...\")\n",
    "            print(f\"   Date: {comment['date']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error extracting comments: {e}\")\n",
    "    comments_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb96ed6",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Note that we now get a visual of the Author, Score, Text, and Date of each comment. The main issue is that we are going comment-by-comment, which are unlikely to have a lot of data in them, unless it's a hot topic. Instead, we want to grab the entire JMU subreddit. This is slightly more complicated, because we want each post and then the subsequent comments. It also means we will get A LOT of data. We want to be able to limit this somehow. Likewise, we don't necessarily want to grab just a random sample but grab things that seem the most relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb615107",
   "metadata": {},
   "source": [
    "# Step 3: Scraping Entire Subreddits\n",
    "\n",
    "The scraper below takes three main variables: \n",
    "- `subreddit_name` - the name of the subreddit without /r\n",
    "- `num_posts` - The number of posts you want to scrape (note that you are limited to 600 posts/minute)\n",
    "- `sort_method` - Options: \"hot\", \"new\", \"top\", \"rising\". Reddit uses these to organize posts\n",
    "\n",
    "You can experiment with the different settings to get a collection of posts that will show up in the output. Keep in mind, how you extract the data determines what data you'll be analyzing. If you are only looking at \"new\" comments, you might miss a major issue in the community. If you only look at \"top\" comments, you'll miss what folks are currently concerned with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79b754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Scraping r/JMU for text analysis\n",
      "📊 Getting 100 top posts...\n",
      "⏳ This may take a minute...\n",
      "📝 Processing post 1/100: President Alger leaving to take same job at Americ...\n",
      "📝 Processing post 1/100: President Alger leaving to take same job at Americ...\n",
      "📝 Processing post 2/100: Alger’s response to hitting over 500 cases in a we...\n",
      "📝 Processing post 3/100: Virginia schools be like...\n",
      "📝 Processing post 2/100: Alger’s response to hitting over 500 cases in a we...\n",
      "📝 Processing post 3/100: Virginia schools be like...\n",
      "📝 Processing post 4/100: The Dukes Advance!...\n",
      "📝 Processing post 5/100: took My graduation pictures today while maintainin...\n",
      "📝 Processing post 4/100: The Dukes Advance!...\n",
      "📝 Processing post 5/100: took My graduation pictures today while maintainin...\n",
      "📝 Processing post 6/100: Taking Senior Photos During JMU Construction (2019...\n",
      "📝 Processing post 7/100: Leaving Hburg today, and this’ll be my last impres...\n",
      "📝 Processing post 6/100: Taking Senior Photos During JMU Construction (2019...\n",
      "📝 Processing post 7/100: Leaving Hburg today, and this’ll be my last impres...\n",
      "📝 Processing post 8/100: Campus Reopening Plan...\n",
      "📝 Processing post 9/100: JMU vs WEBER STATE 8/30/25...\n",
      "📝 Processing post 8/100: Campus Reopening Plan...\n",
      "📝 Processing post 9/100: JMU vs WEBER STATE 8/30/25...\n",
      "📝 Processing post 10/100: JMU images featured in Daily Show’s “Pandumbic” co...\n",
      "📝 Processing post 11/100: Truly a revolutionary concept...\n",
      "📝 Processing post 10/100: JMU images featured in Daily Show’s “Pandumbic” co...\n",
      "📝 Processing post 11/100: Truly a revolutionary concept...\n",
      "📝 Processing post 12/100: Welp...\n",
      "📝 Processing post 12/100: Welp...\n",
      "📝 Processing post 13/100: We did it Patrick! We Re-Opened Campus!...\n",
      "📝 Processing post 14/100: Me when I think about how school is in less than t...\n",
      "📝 Processing post 13/100: We did it Patrick! We Re-Opened Campus!...\n",
      "📝 Processing post 14/100: Me when I think about how school is in less than t...\n",
      "📝 Processing post 15/100: Well that was fast...\n",
      "📝 Processing post 16/100: JMU Nazi Zombies Edit...\n",
      "📝 Processing post 15/100: Well that was fast...\n",
      "📝 Processing post 16/100: JMU Nazi Zombies Edit...\n",
      "📝 Processing post 17/100: .............\n",
      "📝 Processing post 17/100: .............\n",
      "📝 Processing post 18/100: RIP 😥...\n",
      "📝 Processing post 18/100: RIP 😥...\n",
      "📝 Processing post 19/100: Cautious optimism....\n",
      "📝 Processing post 20/100: LUCI UPDATE: SHE’S HOME :) Had a convo with the bu...\n",
      "📝 Processing post 19/100: Cautious optimism....\n",
      "📝 Processing post 20/100: LUCI UPDATE: SHE’S HOME :) Had a convo with the bu...\n",
      "📝 Processing post 21/100: JMU Administration has failed you all. I'm an alum...\n",
      "📝 Processing post 22/100: JMU: \"Signage will warn students against entry if ...\n",
      "📝 Processing post 21/100: JMU Administration has failed you all. I'm an alum...\n",
      "📝 Processing post 22/100: JMU: \"Signage will warn students against entry if ...\n",
      "📝 Processing post 23/100: JMU Fall 2020...\n",
      "📝 Processing post 24/100: Wake up people...\n",
      "📝 Processing post 23/100: JMU Fall 2020...\n",
      "📝 Processing post 24/100: Wake up people...\n",
      "📝 Processing post 25/100: RIP old D-Hall...\n",
      "📝 Processing post 26/100: this is it....\n",
      "📝 Processing post 25/100: RIP old D-Hall...\n",
      "📝 Processing post 26/100: this is it....\n",
      "📝 Processing post 27/100: Serious question: who wore it best?...\n",
      "📝 Processing post 28/100: Are we past halfway the semester already?...\n",
      "📝 Processing post 27/100: Serious question: who wore it best?...\n",
      "📝 Processing post 28/100: Are we past halfway the semester already?...\n",
      "📝 Processing post 29/100: Guess Alger forgot about this...\n",
      "📝 Processing post 30/100: The home of the Dukes in MS Flight Simulator...\n",
      "📝 Processing post 29/100: Guess Alger forgot about this...\n",
      "📝 Processing post 30/100: The home of the Dukes in MS Flight Simulator...\n",
      "📝 Processing post 31/100: Flag Twirlers Let’s Move It...\n",
      "📝 Processing post 32/100: Alger be like:...\n",
      "📝 Processing post 31/100: Flag Twirlers Let’s Move It...\n",
      "📝 Processing post 32/100: Alger be like:...\n",
      "📝 Processing post 33/100: Le upgrade has arrived...\n",
      "📝 Processing post 34/100: president alger once jmu gets their coin...\n",
      "📝 Processing post 33/100: Le upgrade has arrived...\n",
      "📝 Processing post 34/100: president alger once jmu gets their coin...\n",
      "📝 Processing post 35/100: Attention all dukes...\n",
      "📝 Processing post 36/100: “ThE nUmBeR oF nEw PoSiTiVe CaSeS iS nOt A sInGuLa...\n",
      "📝 Processing post 35/100: Attention all dukes...\n",
      "📝 Processing post 36/100: “ThE nUmBeR oF nEw PoSiTiVe CaSeS iS nOt A sInGuLa...\n",
      "📝 Processing post 37/100: Consider: the Duke Dog with no eyebrows...\n",
      "📝 Processing post 38/100: I’m not ready :(...\n",
      "📝 Processing post 37/100: Consider: the Duke Dog with no eyebrows...\n",
      "📝 Processing post 38/100: I’m not ready :(...\n",
      "📝 Processing post 39/100: Major donor & alum calls for Board to not renew Al...\n",
      "📝 Processing post 39/100: Major donor & alum calls for Board to not renew Al...\n",
      "📝 Processing post 40/100: I still have all four of my D-Hall \"recycle\" mugs....\n",
      "📝 Processing post 41/100: If you're in an overcrowded classroom, TAKE PHOTOS...\n",
      "📝 Processing post 40/100: I still have all four of my D-Hall \"recycle\" mugs....\n",
      "📝 Processing post 41/100: If you're in an overcrowded classroom, TAKE PHOTOS...\n",
      "📝 Processing post 42/100: JMU PUTS 70 ON UNC...\n",
      "📝 Processing post 43/100: If you didn’t read JMU’s email… here’s a shortened...\n",
      "📝 Processing post 42/100: JMU PUTS 70 ON UNC...\n",
      "📝 Processing post 43/100: If you didn’t read JMU’s email… here’s a shortened...\n",
      "📝 Processing post 44/100: Me every day this past week...\n",
      "📝 Processing post 45/100: Justice Studies Prof Died of Covid 3 days ago...\n",
      "📝 Processing post 44/100: Me every day this past week...\n",
      "📝 Processing post 45/100: Justice Studies Prof Died of Covid 3 days ago...\n",
      "📝 Processing post 46/100: I’m happy to say that I’m going to be a Duke next ...\n",
      "📝 Processing post 47/100: course evaluations be like...\n",
      "📝 Processing post 46/100: I’m happy to say that I’m going to be a Duke next ...\n",
      "📝 Processing post 47/100: course evaluations be like...\n",
      "📝 Processing post 48/100: Housing Info...\n",
      "📝 Processing post 48/100: Housing Info...\n",
      "📝 Processing post 49/100: I give it a week...\n",
      "📝 Processing post 50/100: The Festival atrium in the early evening on the fi...\n",
      "📝 Processing post 49/100: I give it a week...\n",
      "📝 Processing post 50/100: The Festival atrium in the early evening on the fi...\n",
      "📝 Processing post 51/100: Professor Ill be late to class. The Evergiven is b...\n",
      "📝 Processing post 52/100: @the selfish people who plan to party......\n",
      "📝 Processing post 51/100: Professor Ill be late to class. The Evergiven is b...\n",
      "📝 Processing post 52/100: @the selfish people who plan to party......\n",
      "📝 Processing post 53/100: Left Aid house...\n",
      "📝 Processing post 54/100: Petition to ask Alger to fight the angry Bible man...\n",
      "📝 Processing post 53/100: Left Aid house...\n",
      "📝 Processing post 54/100: Petition to ask Alger to fight the angry Bible man...\n",
      "📝 Processing post 55/100: Hey guys, quick psa, I have started to see these o...\n",
      "📝 Processing post 55/100: Hey guys, quick psa, I have started to see these o...\n",
      "📝 Processing post 56/100: Roll Dukes ✊😔...\n",
      "📝 Processing post 57/100: “Can we get a lowered tuition rate if there is no ...\n",
      "📝 Processing post 56/100: Roll Dukes ✊😔...\n",
      "📝 Processing post 57/100: “Can we get a lowered tuition rate if there is no ...\n",
      "📝 Processing post 58/100: I Made Sure to be Repping in Animal Crossing...\n",
      "📝 Processing post 59/100: Does someone know the context to this pic?...\n",
      "📝 Processing post 58/100: I Made Sure to be Repping in Animal Crossing...\n",
      "📝 Processing post 59/100: Does someone know the context to this pic?...\n",
      "📝 Processing post 60/100: We have officially been ranked!...\n",
      "📝 Processing post 60/100: We have officially been ranked!...\n",
      "📝 Processing post 61/100: JMU upsets UVA, 52-49...\n",
      "📝 Processing post 62/100: JMU Men’s Basketball Sun Belt Conference Champions...\n",
      "📝 Processing post 61/100: JMU upsets UVA, 52-49...\n",
      "📝 Processing post 62/100: JMU Men’s Basketball Sun Belt Conference Champions...\n",
      "📝 Processing post 63/100: We’re screwed...\n",
      "📝 Processing post 63/100: We’re screwed...\n",
      "📝 Processing post 64/100: JMU 1-0!!!...\n",
      "📝 Processing post 65/100: Number 1 school for cases in VA. Roll dukes 😝...\n",
      "📝 Processing post 64/100: JMU 1-0!!!...\n",
      "📝 Processing post 65/100: Number 1 school for cases in VA. Roll dukes 😝...\n",
      "📝 Processing post 66/100: Eagle field last night. No social Distancing, Appr...\n",
      "📝 Processing post 66/100: Eagle field last night. No social Distancing, Appr...\n",
      "📝 Processing post 67/100: Virginia football in a nutshell...\n",
      "📝 Processing post 67/100: Virginia football in a nutshell...\n",
      "📝 Processing post 68/100: The spontaneous protests have returned!...\n",
      "📝 Processing post 68/100: The spontaneous protests have returned!...\n",
      "📝 Processing post 69/100: JMU offers CR/NC but you can’t use it for Major, M...\n",
      "📝 Processing post 69/100: JMU offers CR/NC but you can’t use it for Major, M...\n",
      "📝 Processing post 70/100: Our little quarantine project :)...\n",
      "📝 Processing post 71/100: My feelings exactly...\n",
      "📝 Processing post 70/100: Our little quarantine project :)...\n",
      "📝 Processing post 71/100: My feelings exactly...\n",
      "📝 Processing post 72/100: Anybody give a rundown of what happened at that hy...\n",
      "📝 Processing post 72/100: Anybody give a rundown of what happened at that hy...\n",
      "📝 Processing post 73/100: We lost a future Duke, y'all...\n",
      "📝 Processing post 73/100: We lost a future Duke, y'all...\n",
      "📝 Processing post 74/100: I feel this on a different level...\n",
      "📝 Processing post 74/100: I feel this on a different level...\n",
      "📝 Processing post 75/100: Pour one out for our injured canine...\n",
      "📝 Processing post 76/100: Flash mob rave at ECL in 2009...\n",
      "📝 Processing post 75/100: Pour one out for our injured canine...\n",
      "📝 Processing post 76/100: Flash mob rave at ECL in 2009...\n",
      "📝 Processing post 77/100: JMU can fuck right off with their \"Giving Week\" em...\n",
      "📝 Processing post 77/100: JMU can fuck right off with their \"Giving Week\" em...\n",
      "📝 Processing post 78/100: Just incase you need a good laugh...\n",
      "📝 Processing post 79/100: Me tonight in my pjs eating my Boorito bowl and st...\n",
      "📝 Processing post 78/100: Just incase you need a good laugh...\n",
      "📝 Processing post 79/100: Me tonight in my pjs eating my Boorito bowl and st...\n",
      "📝 Processing post 80/100: “Warm Regards” ~ Alger...\n",
      "📝 Processing post 81/100: Stay Safe - Don't go back...\n",
      "📝 Processing post 80/100: “Warm Regards” ~ Alger...\n",
      "📝 Processing post 81/100: Stay Safe - Don't go back...\n",
      "📝 Processing post 82/100: Well done!...\n",
      "📝 Processing post 82/100: Well done!...\n",
      "📝 Processing post 83/100: word of advice: don’t buy parking passes/meal plan...\n",
      "📝 Processing post 83/100: word of advice: don’t buy parking passes/meal plan...\n",
      "📝 Processing post 84/100: When I'm going to the library but I see the clubs ...\n",
      "📝 Processing post 85/100: Cursed cookout...\n",
      "📝 Processing post 84/100: When I'm going to the library but I see the clubs ...\n",
      "📝 Processing post 85/100: Cursed cookout...\n",
      "📝 Processing post 86/100: Last night, Ben DiNucci became the first-ever JMU ...\n",
      "📝 Processing post 87/100: Monitor someone put on the quad...\n",
      "📝 Processing post 86/100: Last night, Ben DiNucci became the first-ever JMU ...\n",
      "📝 Processing post 87/100: Monitor someone put on the quad...\n",
      "📝 Processing post 88/100: happy assessment day...\n",
      "📝 Processing post 89/100: Make good decisions tonight Dukes :)...\n",
      "📝 Processing post 88/100: happy assessment day...\n",
      "📝 Processing post 89/100: Make good decisions tonight Dukes :)...\n",
      "📝 Processing post 90/100: JMU Eliminates DEI...\n",
      "📝 Processing post 90/100: JMU Eliminates DEI...\n",
      "📝 Processing post 91/100: anyone know what's happening?...\n",
      "📝 Processing post 91/100: anyone know what's happening?...\n",
      "📝 Processing post 92/100: you guys fell for JMU's propaganda...\n",
      "📝 Processing post 92/100: you guys fell for JMU's propaganda...\n",
      "📝 Processing post 93/100: when you register for your last gen ed...\n",
      "📝 Processing post 94/100: JMU changes withdraw deadline to October 10th....\n",
      "📝 Processing post 93/100: when you register for your last gen ed...\n",
      "📝 Processing post 94/100: JMU changes withdraw deadline to October 10th....\n",
      "📝 Processing post 95/100: Happened right on the quad...\n",
      "📝 Processing post 95/100: Happened right on the quad...\n",
      "📝 Processing post 96/100: Photos of the Quad Cats in their new home!...\n",
      "📝 Processing post 97/100: Quad Cat Update!...\n",
      "📝 Processing post 96/100: Photos of the Quad Cats in their new home!...\n",
      "📝 Processing post 97/100: Quad Cat Update!...\n",
      "📝 Processing post 98/100: Be aware of this man…multiple people have already ...\n",
      "📝 Processing post 98/100: Be aware of this man…multiple people have already ...\n",
      "📝 Processing post 99/100: A bunch of photos that I took on Sunday, November ...\n",
      "📝 Processing post 100/100: If anyone is the Harrisonburg area needs food, ple...\n",
      "📝 Processing post 99/100: A bunch of photos that I took on Sunday, November ...\n",
      "📝 Processing post 100/100: If anyone is the Harrisonburg area needs food, ple...\n",
      "\n",
      "✅ Successfully collected 1537 text items!\n",
      "📊 Ready for text analysis from r/JMU\n",
      "\n",
      "✅ Successfully collected 1537 text items!\n",
      "📊 Ready for text analysis from r/JMU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subreddit_name = \"JMU\"  # Change this to any subreddit (without r/)\n",
    "num_posts = 100         # How many posts to scrape\n",
    "sort_method = \"top\"     # Options: \"hot\", \"new\", \"top\", \"rising\"\n",
    "\n",
    "print(f\"🎯 Scraping r/{subreddit_name} for text analysis\")\n",
    "print(f\"📊 Getting {num_posts} {sort_method} posts...\")\n",
    "print(\"⏳ This may take a minute...\")\n",
    "\n",
    "# Simple data structure - no duplicates\n",
    "text_data = []\n",
    "\n",
    "try:\n",
    "    # Get the subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Choose sorting method\n",
    "    if sort_method == \"hot\":\n",
    "        posts = subreddit.hot(limit=num_posts)\n",
    "    elif sort_method == \"new\":\n",
    "        posts = subreddit.new(limit=num_posts)\n",
    "    elif sort_method == \"top\":\n",
    "        posts = subreddit.top(limit=num_posts)\n",
    "    elif sort_method == \"rising\":\n",
    "        posts = subreddit.rising(limit=num_posts)\n",
    "    else:\n",
    "        posts = subreddit.hot(limit=num_posts)\n",
    "    \n",
    "    # Loop through each post\n",
    "    for post_num, submission in enumerate(posts, 1):\n",
    "        print(f\"📝 Processing post {post_num}/{num_posts}: {submission.title[:50]}...\")\n",
    "        \n",
    "        # Get post date for reference\n",
    "        post_date = datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Add the post itself (title + content)\n",
    "        post_text = submission.title\n",
    "        if submission.selftext.strip():  # Add post content if it exists\n",
    "            post_text += \" \" + submission.selftext\n",
    "            \n",
    "        text_data.append({\n",
    "            'type': 'post',\n",
    "            'title': submission.title,\n",
    "            'text': post_text,\n",
    "            'date': post_date,\n",
    "            'score': submission.score\n",
    "        })\n",
    "        \n",
    "        # Get comments for this post\n",
    "        try:\n",
    "            submission.comments.replace_more(limit=0)  # Don't expand \"more comments\"\n",
    "            \n",
    "            for comment in submission.comments.list()[:50]:  # Get more comments per post\n",
    "                if hasattr(comment, 'body') and comment.body.strip():\n",
    "                    comment_date = datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    \n",
    "                    text_data.append({\n",
    "                        'type': 'comment',\n",
    "                        'title': submission.title,  # Thread title for reference\n",
    "                        'text': comment.body,\n",
    "                        'date': comment_date,\n",
    "                        'score': comment.score\n",
    "                    })\n",
    "        except:\n",
    "            print(f\"   ⚠️ Could not load comments for this post\")\n",
    "    \n",
    "    print(f\"\\n✅ Successfully collected {len(text_data)} text items!\")\n",
    "    print(f\"📊 Ready for text analysis from r/{subreddit_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error scraping subreddit: {e}\")\n",
    "    print(\"💡 Make sure the subreddit name is correct and accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab304ac",
   "metadata": {},
   "source": [
    "# Step 4: Generate Output\n",
    "\n",
    "Once the data has been created, you want to write that to a file. The script below creates two different files:\n",
    "\n",
    "- `reddit_text_analysis...csv`\n",
    "- `reddit_voyant.txt`\n",
    "\n",
    "We will use both files eventually. The `.csv` file is a table of all the comments and posts and the `.txt` file is the same information, but only as text, which is useful for visualizing in Voyant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18831ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 TEXT ANALYSIS STATS:\n",
      "   • Posts: 95\n",
      "   • Comments: 1285\n",
      "   • Total words: 41,568\n",
      "   • Average text length: 173 characters\n",
      "   • Longest text: 7651 characters\n",
      "\n",
      "📋 SAMPLE TEXTS:\n",
      "1. [POST] President Alger leaving to take same job at American University at the end of this academic year...\n",
      "2. [COMMENT] Like him or not, he did help transform this school. Applications to JMU have drastically increased u...\n",
      "3. [COMMENT] Massive changes happening at JMU this year. Alger stepping down, AD Bourne retiring, Cignetti left f...\n",
      "\n",
      "🎯 Created Voyant-ready file: data\\reddit_voyant_JMU_20250930_212751.txt\n"
     ]
    }
   ],
   "source": [
    "# 💾 Save Clean Text Data for Topic Modeling\n",
    "\n",
    "if text_data:\n",
    "    try:\n",
    "        # Create data folder if it doesn't exist\n",
    "        data_folder = \"data\"\n",
    "        os.makedirs(data_folder, exist_ok=True)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df_text = pd.DataFrame(text_data)\n",
    "        \n",
    "        # Clean text function for analysis\n",
    "        def clean_text_for_analysis(text):\n",
    "            if pd.isna(text):\n",
    "                return \"\"\n",
    "            text = str(text)\n",
    "            # Handle encoding\n",
    "            text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            # Remove control characters but keep newlines\n",
    "            text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\r\\t')\n",
    "            # Remove extra whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            return text\n",
    "        \n",
    "        # Clean the text data\n",
    "        df_text['text'] = df_text['text'].apply(clean_text_for_analysis)\n",
    "        df_text['title'] = df_text['title'].apply(clean_text_for_analysis)\n",
    "        \n",
    "        # Remove empty entries\n",
    "        df_text = df_text[df_text['text'].str.len() > 10]  # Remove very short texts\n",
    "        \n",
    "        # Ensure proper data types\n",
    "        df_text['score'] = pd.to_numeric(df_text['score'], errors='coerce')\n",
    "        df_text['date'] = pd.to_datetime(df_text['date'], errors='coerce')\n",
    "        df_text['type'] = df_text['type'].astype('category')\n",
    "        \n",
    "        # Create filename for analysis (save to data folder)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = os.path.join(data_folder, f\"reddit_text_analysis_{subreddit_name}_{sort_method}_num_posts_{num_posts}_{timestamp}.csv\")\n",
    "        \n",
    "        # Save to CSV optimized for text analysis\n",
    "        df_text.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "      \n",
    "          \n",
    "        \n",
    "        \n",
    "     \n",
    "        # Show statistics for text analysis\n",
    "        posts_count = (df_text['type'] == 'post').sum()\n",
    "        comments_count = (df_text['type'] == 'comment').sum()\n",
    "        \n",
    "        print(f\"\\n📈 TEXT ANALYSIS STATS:\")\n",
    "        print(f\"   • Posts: {posts_count}\")\n",
    "        print(f\"   • Comments: {comments_count}\")\n",
    "        print(f\"   • Total words: {df_text['text'].str.split().str.len().sum():,}\")\n",
    "        print(f\"   • Average text length: {df_text['text'].str.len().mean():.0f} characters\")\n",
    "        print(f\"   • Longest text: {df_text['text'].str.len().max()} characters\")\n",
    "        \n",
    "        print(f\"\\n📋 SAMPLE TEXTS:\")\n",
    "        for idx, row in enumerate(df_text.head(3).iterrows(), 1):\n",
    "            _, row_data = row\n",
    "            print(f\"{idx}. [{row_data['type'].upper()}] {row_data['text'][:100]}...\")\n",
    "        \n",
    "        # Create a separate file with just the text for Voyant (save to data folder)\n",
    "        text_only_filename = os.path.join(data_folder, f\"reddit_voyant_{subreddit_name}_{timestamp}.txt\")\n",
    "        with open(text_only_filename, 'w', encoding='utf-8') as f:\n",
    "            for _, row in df_text.iterrows():\n",
    "                f.write(f\"{row['text']}\\n\\n\")\n",
    "        \n",
    "        print(f\"\\n🎯 Created Voyant-ready file: {text_only_filename}\")\n",
    "        \n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving text data: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No text data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f8e15",
   "metadata": {},
   "source": [
    "## 🎯 Simplified Text Scraper for Topic Modeling\n",
    "\n",
    "### 📝 What This Does:\n",
    "This streamlined version focuses on **clean text extraction** for analysis tools like Voyant:\n",
    "- **No duplicates**: Each piece of text appears only once\n",
    "- **Clean format**: Text optimized for analysis\n",
    "- **Two outputs**: CSV for data analysis + TXT file for Voyant\n",
    "\n",
    "### \udd27 Quick Setup:\n",
    "1. **Change subreddit**: `subreddit_name = \"JMU\"`\n",
    "2. **Set post count**: `num_posts = 100` \n",
    "3. **Choose sorting**: `sort_method = \"hot\"`\n",
    "4. **Run both cells above**\n",
    "\n",
    "### \udcca What You Get:\n",
    "\n",
    "#### CSV File Contains:\n",
    "- **type**: \"post\" or \"comment\"\n",
    "- **title**: Thread title (for context)\n",
    "- **text**: The actual text content\n",
    "- **date**: When it was posted\n",
    "- **score**: Reddit score\n",
    "\n",
    "#### TXT File Contains:\n",
    "- Pure text, one item per line\n",
    "- Perfect for uploading directly to Voyant\n",
    "- No metadata, just content\n",
    "\n",
    "### 🎯 Perfect for Topic Modeling:\n",
    "- **Voyant Tools**: Upload the .txt file directly\n",
    "- **Other text analysis**: Use the .csv file\n",
    "- **Clean data**: Removed duplicates and empty entries\n",
    "- **Focused content**: Just the text you need\n",
    "\n",
    "### 💡 Analysis Tips:\n",
    "- **Voyant**: Use the .txt file for word clouds, trends, correlations\n",
    "- **Text length**: Longer texts work better for topic modeling\n",
    "- **Sample size**: 50-100 posts usually gives good results\n",
    "- **Time periods**: Try different sorting methods to see trends\n",
    "\n",
    "This approach gives you exactly what you need for text analysis without the complexity!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds101_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
