{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c2c03e9",
   "metadata": {},
   "source": [
    "# Lesson 1:  Reddit Comment Scraper\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "1. **Extract comments** from any Reddit thread\n",
    "2. **Change the target URL** to scrape different discussions\n",
    "4. **Change extraction quantity** to scrape more or less data\n",
    "3. **Save data to CSV** for further analysis\n",
    "\n",
    "\n",
    "## üöÄ What You'll Build\n",
    "A simple but powerful Reddit comment scraper that can:\n",
    "- Take any Reddit thread URL\n",
    "- Extract all comments with metadata (author, score, text, timestamp)\n",
    "- Save the results to a CSV file for use in other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8434bc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n",
      "üì¶ Ready to scrape Reddit comments\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(\"üì¶ Ready to scrape Reddit comments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d4662",
   "metadata": {},
   "source": [
    "## Reddit API Authentication\n",
    "\n",
    "### Overview\n",
    "Reddit allows users to \"scrape\" data from their website using an API (Application Programming Interface). Because \"scraping\" can be taxing on their servers, you have to authenticate as a user to scrape more posts per minute. The script below authenticates you as a user so you can \"scrape\" (download posts) at a higher limit. If you are not authenticated, you can still get data, but it's slower.\n",
    "\n",
    "### üéØ Two Authentication Methods:\n",
    "\n",
    "| Method | Rate Limit | Best For |\n",
    "|--------|------------|----------|\n",
    "| **Authenticated (Encrypted)** | 600 requests/minute | Large subreddits, many posts |\n",
    "| **Anonymous (Read-only)** | 60 requests/minute | Single threads, small datasets |\n",
    "\n",
    "### üîí Security:\n",
    "I have set up **encrypted credentials** that give you higher rate limits while keeping the actual login details secure. All of this authentication code has been hidden from you in a separate module.\n",
    "\n",
    "### üîß Setup:\n",
    "The authentication cell below will **automatically** choose the best available method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7e4d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connecting to Reddit...\n",
      "‚úÖ Read-only connection ready! (60 requests/minute)\n",
      "üéØ Ready to scrape!\n",
      "‚úÖ Read-only connection ready! (60 requests/minute)\n",
      "üéØ Ready to scrape!\n"
     ]
    }
   ],
   "source": [
    "# üîß Setup Reddit Connection\n",
    "\n",
    "# Clear any previous imports to avoid caching issues\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Add the config directory to the Python path\n",
    "import os\n",
    "config_path = os.path.join(os.getcwd(), 'config')\n",
    "if config_path not in sys.path:\n",
    "    sys.path.insert(0, config_path)\n",
    "\n",
    "# Clear cached modules\n",
    "if 'reddit_auth' in sys.modules:\n",
    "    importlib.reload(sys.modules['reddit_auth'])\n",
    "\n",
    "from reddit_auth import setup_reddit_connection\n",
    "\n",
    "print(\"üîó Connecting to Reddit...\")\n",
    "\n",
    "# Single function call with explicit variable assignment\n",
    "reddit, auth_mode, rate_limit = setup_reddit_connection()\n",
    "\n",
    "# Single, clear status message\n",
    "status_msg = \"‚úÖ Authenticated connection ready! (600 requests/minute)\" if auth_mode == \"authenticated\" else \"‚úÖ Read-only connection ready! (60 requests/minute)\"\n",
    "print(status_msg)\n",
    "print(\"üéØ Ready to scrape!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1232a9f",
   "metadata": {},
   "source": [
    "## STEP 1: Choose Your Reddit Thread\n",
    "\n",
    "To find a thread URL:\n",
    "1. Go to Reddit.com\n",
    "2. Find an interesting post with lots of comments\n",
    "3. Copy the full URL from your browser\n",
    "4. Paste it below (replace the current URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e3f3fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Target URL: https://www.reddit.com/r/jmu/comments/1lbrjnx/best_jmu_suitestyle_halls_need_help_ranking_area/\n",
      "üìù To change this, modify the 'url' variable above\n",
      "\n",
      "‚úÖ Successfully loaded thread!\n",
      "üì∞ Title: 'Best JMU suite-style halls. Need help ranking area preferences (1-8) for housing'\n",
      "üìä Score: 4\n",
      "üí¨ Comments: 14\n",
      "üìÖ Subreddit: r/jmu\n"
     ]
    }
   ],
   "source": [
    "#Change this URL\n",
    "url = \"https://www.reddit.com/r/jmu/comments/1lbrjnx/best_jmu_suitestyle_halls_need_help_ranking_area/\"\n",
    "\n",
    "print(f\"üîó Target URL: {url}\")\n",
    "print(\"üìù To change this, modify the 'url' variable above\")\n",
    "\n",
    "# Load the Reddit thread\n",
    "try:\n",
    "    submission = reddit.submission(url=url)\n",
    "    print(f\"\\n‚úÖ Successfully loaded thread!\")\n",
    "    print(f\"üì∞ Title: '{submission.title}'\")\n",
    "    print(f\"üìä Score: {submission.score}\")\n",
    "    print(f\"üí¨ Comments: {submission.num_comments}\")\n",
    "    print(f\"üìÖ Subreddit: r/{submission.subreddit}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading thread: {e}\")\n",
    "    print(\"üí° Make sure the URL is a valid Reddit thread URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914c795",
   "metadata": {},
   "source": [
    "## STEP 2: Extract All Comments from the Thread\n",
    "\n",
    "Once you have downloaded all of the comments, these are bundled in a special format. The comments need to be \"extracted\" into something humans can read. The function below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "045ad06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting comments from the thread...\n",
      "‚è≥ This may take a few seconds for threads with many comments...\n",
      "‚úÖ Successfully extracted 5 comments!\n",
      "\n",
      "üìã Preview of first 3 comments:\n",
      "\n",
      "1. Author: Pitiful-Pickle-5101 | Score: 2\n",
      "   Text: Do you mean Jack and Jill style bathrooms? Or like village suite style where 3 sets of roommates sha...\n",
      "   Date: 2025-06-15 14:56:40\n",
      "\n",
      "2. Author: An51759 | Score: 1\n",
      "   Text: Also I heard assignment is random, I was wondering how I‚Äôll be able to pick a suite?...\n",
      "   Date: 2025-06-15 13:13:36\n",
      "\n",
      "3. Author: flutiful_fiona | Score: 1\n",
      "   Text: the village dorms are suite style, but don't live there unless you like to party. the bathrooms get ...\n",
      "   Date: 2025-06-20 13:02:33\n"
     ]
    }
   ],
   "source": [
    "# üîç STEP 2: Extract All Comments from the Thread\n",
    "\n",
    "print(\"üîç Extracting comments from the thread...\")\n",
    "print(\"‚è≥ This may take a few seconds for threads with many comments...\")\n",
    "\n",
    "comments_data = []\n",
    "\n",
    "try:\n",
    "    # Loop through all top-level comments\n",
    "    for comment in submission.comments:\n",
    "        if hasattr(comment, 'body'):  # Skip \"MoreComments\" objects\n",
    "            # Convert timestamp to readable date\n",
    "            comment_date = datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            # Store comment information\n",
    "            comment_info = {\n",
    "                'author': str(comment.author) if comment.author else '[deleted]',\n",
    "                'score': comment.score,\n",
    "                'text': comment.body,\n",
    "                'date': comment_date,\n",
    "                'thread_title': submission.title,\n",
    "                'subreddit': str(submission.subreddit)\n",
    "            }\n",
    "            comments_data.append(comment_info)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully extracted {len(comments_data)} comments!\")\n",
    "    \n",
    "    # Show a preview of the first few comments\n",
    "    if comments_data:\n",
    "        print(f\"\\nüìã Preview of first 3 comments:\")\n",
    "        for i, comment in enumerate(comments_data[:3], 1):\n",
    "            print(f\"\\n{i}. Author: {comment['author']} | Score: {comment['score']}\")\n",
    "            print(f\"   Text: {comment['text'][:100]}...\")\n",
    "            print(f\"   Date: {comment['date']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error extracting comments: {e}\")\n",
    "    comments_data = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb96ed6",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "Note that we now get a visual of the Author, Score, Text, and Date of each comment. The main issue is that we are going comment-by-comment, which are unlikely to have a lot of data in them, unless it's a hot topic. Instead, we want to grab the entire JMU subreddit. This is slightly more complicated, because we want each post and then the subsequent comments. It also means we will get A LOT of data. We want to be able to limit this somehow. Likewise, we don't necessarily want to grab just a random sample but grab things that seem the most relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb615107",
   "metadata": {},
   "source": [
    "# Step 3: Scraping Entire Subreddits\n",
    "\n",
    "The scraper below takes three main variables: \n",
    "- `subreddit_name` - the name of the subreddit without /r\n",
    "- `num_posts` - The number of posts you want to scrape (note that you are limited to 600 posts/minute)\n",
    "- `sort_method` - Options: \"hot\", \"new\", \"top\", \"rising\". Reddit uses these to organize posts\n",
    "\n",
    "You can experiment with the different settings to get a collection of posts that will show up in the output. Keep in mind, how you extract the data determines what data you'll be analyzing. If you are only looking at \"new\" comments, you might miss a major issue in the community. If you only look at \"top\" comments, you'll miss what folks are currently concerned with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a79b754a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Scraping r/JMU for text analysis\n",
      "üìä Getting 100 top posts...\n",
      "‚è≥ This may take a minute...\n",
      "üìù Processing post 1/100: President Alger leaving to take same job at Americ...\n",
      "üìù Processing post 1/100: President Alger leaving to take same job at Americ...\n",
      "üìù Processing post 2/100: Alger‚Äôs response to hitting over 500 cases in a we...\n",
      "üìù Processing post 3/100: Virginia schools be like...\n",
      "üìù Processing post 2/100: Alger‚Äôs response to hitting over 500 cases in a we...\n",
      "üìù Processing post 3/100: Virginia schools be like...\n",
      "üìù Processing post 4/100: The Dukes Advance!...\n",
      "üìù Processing post 5/100: took My graduation pictures today while maintainin...\n",
      "üìù Processing post 4/100: The Dukes Advance!...\n",
      "üìù Processing post 5/100: took My graduation pictures today while maintainin...\n",
      "üìù Processing post 6/100: Taking Senior Photos During JMU Construction (2019...\n",
      "üìù Processing post 7/100: Leaving Hburg today, and this‚Äôll be my last impres...\n",
      "üìù Processing post 6/100: Taking Senior Photos During JMU Construction (2019...\n",
      "üìù Processing post 7/100: Leaving Hburg today, and this‚Äôll be my last impres...\n",
      "üìù Processing post 8/100: Campus Reopening Plan...\n",
      "üìù Processing post 9/100: JMU vs WEBER STATE 8/30/25...\n",
      "üìù Processing post 8/100: Campus Reopening Plan...\n",
      "üìù Processing post 9/100: JMU vs WEBER STATE 8/30/25...\n",
      "üìù Processing post 10/100: JMU images featured in Daily Show‚Äôs ‚ÄúPandumbic‚Äù co...\n",
      "üìù Processing post 11/100: Truly a revolutionary concept...\n",
      "üìù Processing post 10/100: JMU images featured in Daily Show‚Äôs ‚ÄúPandumbic‚Äù co...\n",
      "üìù Processing post 11/100: Truly a revolutionary concept...\n",
      "üìù Processing post 12/100: Welp...\n",
      "üìù Processing post 12/100: Welp...\n",
      "üìù Processing post 13/100: We did it Patrick! We Re-Opened Campus!...\n",
      "üìù Processing post 14/100: Me when I think about how school is in less than t...\n",
      "üìù Processing post 13/100: We did it Patrick! We Re-Opened Campus!...\n",
      "üìù Processing post 14/100: Me when I think about how school is in less than t...\n",
      "üìù Processing post 15/100: Well that was fast...\n",
      "üìù Processing post 16/100: JMU Nazi Zombies Edit...\n",
      "üìù Processing post 15/100: Well that was fast...\n",
      "üìù Processing post 16/100: JMU Nazi Zombies Edit...\n",
      "üìù Processing post 17/100: .............\n",
      "üìù Processing post 17/100: .............\n",
      "üìù Processing post 18/100: RIP üò•...\n",
      "üìù Processing post 18/100: RIP üò•...\n",
      "üìù Processing post 19/100: Cautious optimism....\n",
      "üìù Processing post 20/100: LUCI UPDATE: SHE‚ÄôS HOME :) Had a convo with the bu...\n",
      "üìù Processing post 19/100: Cautious optimism....\n",
      "üìù Processing post 20/100: LUCI UPDATE: SHE‚ÄôS HOME :) Had a convo with the bu...\n",
      "üìù Processing post 21/100: JMU Administration has failed you all. I'm an alum...\n",
      "üìù Processing post 22/100: JMU: \"Signage will warn students against entry if ...\n",
      "üìù Processing post 21/100: JMU Administration has failed you all. I'm an alum...\n",
      "üìù Processing post 22/100: JMU: \"Signage will warn students against entry if ...\n",
      "üìù Processing post 23/100: JMU Fall 2020...\n",
      "üìù Processing post 24/100: Wake up people...\n",
      "üìù Processing post 23/100: JMU Fall 2020...\n",
      "üìù Processing post 24/100: Wake up people...\n",
      "üìù Processing post 25/100: RIP old D-Hall...\n",
      "üìù Processing post 26/100: this is it....\n",
      "üìù Processing post 25/100: RIP old D-Hall...\n",
      "üìù Processing post 26/100: this is it....\n",
      "üìù Processing post 27/100: Serious question: who wore it best?...\n",
      "üìù Processing post 28/100: Are we past halfway the semester already?...\n",
      "üìù Processing post 27/100: Serious question: who wore it best?...\n",
      "üìù Processing post 28/100: Are we past halfway the semester already?...\n",
      "üìù Processing post 29/100: Guess Alger forgot about this...\n",
      "üìù Processing post 30/100: The home of the Dukes in MS Flight Simulator...\n",
      "üìù Processing post 29/100: Guess Alger forgot about this...\n",
      "üìù Processing post 30/100: The home of the Dukes in MS Flight Simulator...\n",
      "üìù Processing post 31/100: Flag Twirlers Let‚Äôs Move It...\n",
      "üìù Processing post 32/100: Alger be like:...\n",
      "üìù Processing post 31/100: Flag Twirlers Let‚Äôs Move It...\n",
      "üìù Processing post 32/100: Alger be like:...\n",
      "üìù Processing post 33/100: Le upgrade has arrived...\n",
      "üìù Processing post 34/100: president alger once jmu gets their coin...\n",
      "üìù Processing post 33/100: Le upgrade has arrived...\n",
      "üìù Processing post 34/100: president alger once jmu gets their coin...\n",
      "üìù Processing post 35/100: Attention all dukes...\n",
      "üìù Processing post 36/100: ‚ÄúThE nUmBeR oF nEw PoSiTiVe CaSeS iS nOt A sInGuLa...\n",
      "üìù Processing post 35/100: Attention all dukes...\n",
      "üìù Processing post 36/100: ‚ÄúThE nUmBeR oF nEw PoSiTiVe CaSeS iS nOt A sInGuLa...\n",
      "üìù Processing post 37/100: Consider: the Duke Dog with no eyebrows...\n",
      "üìù Processing post 38/100: I‚Äôm not ready :(...\n",
      "üìù Processing post 37/100: Consider: the Duke Dog with no eyebrows...\n",
      "üìù Processing post 38/100: I‚Äôm not ready :(...\n",
      "üìù Processing post 39/100: Major donor & alum calls for Board to not renew Al...\n",
      "üìù Processing post 39/100: Major donor & alum calls for Board to not renew Al...\n",
      "üìù Processing post 40/100: I still have all four of my D-Hall \"recycle\" mugs....\n",
      "üìù Processing post 41/100: If you're in an overcrowded classroom, TAKE PHOTOS...\n",
      "üìù Processing post 40/100: I still have all four of my D-Hall \"recycle\" mugs....\n",
      "üìù Processing post 41/100: If you're in an overcrowded classroom, TAKE PHOTOS...\n",
      "üìù Processing post 42/100: JMU PUTS 70 ON UNC...\n",
      "üìù Processing post 43/100: If you didn‚Äôt read JMU‚Äôs email‚Ä¶ here‚Äôs a shortened...\n",
      "üìù Processing post 42/100: JMU PUTS 70 ON UNC...\n",
      "üìù Processing post 43/100: If you didn‚Äôt read JMU‚Äôs email‚Ä¶ here‚Äôs a shortened...\n",
      "üìù Processing post 44/100: Me every day this past week...\n",
      "üìù Processing post 45/100: Justice Studies Prof Died of Covid 3 days ago...\n",
      "üìù Processing post 44/100: Me every day this past week...\n",
      "üìù Processing post 45/100: Justice Studies Prof Died of Covid 3 days ago...\n",
      "üìù Processing post 46/100: I‚Äôm happy to say that I‚Äôm going to be a Duke next ...\n",
      "üìù Processing post 47/100: course evaluations be like...\n",
      "üìù Processing post 46/100: I‚Äôm happy to say that I‚Äôm going to be a Duke next ...\n",
      "üìù Processing post 47/100: course evaluations be like...\n",
      "üìù Processing post 48/100: Housing Info...\n",
      "üìù Processing post 48/100: Housing Info...\n",
      "üìù Processing post 49/100: I give it a week...\n",
      "üìù Processing post 50/100: The Festival atrium in the early evening on the fi...\n",
      "üìù Processing post 49/100: I give it a week...\n",
      "üìù Processing post 50/100: The Festival atrium in the early evening on the fi...\n",
      "üìù Processing post 51/100: Professor Ill be late to class. The Evergiven is b...\n",
      "üìù Processing post 52/100: @the selfish people who plan to party......\n",
      "üìù Processing post 51/100: Professor Ill be late to class. The Evergiven is b...\n",
      "üìù Processing post 52/100: @the selfish people who plan to party......\n",
      "üìù Processing post 53/100: Left Aid house...\n",
      "üìù Processing post 54/100: Petition to ask Alger to fight the angry Bible man...\n",
      "üìù Processing post 53/100: Left Aid house...\n",
      "üìù Processing post 54/100: Petition to ask Alger to fight the angry Bible man...\n",
      "üìù Processing post 55/100: Hey guys, quick psa, I have started to see these o...\n",
      "üìù Processing post 55/100: Hey guys, quick psa, I have started to see these o...\n",
      "üìù Processing post 56/100: Roll Dukes ‚úäüòî...\n",
      "üìù Processing post 57/100: ‚ÄúCan we get a lowered tuition rate if there is no ...\n",
      "üìù Processing post 56/100: Roll Dukes ‚úäüòî...\n",
      "üìù Processing post 57/100: ‚ÄúCan we get a lowered tuition rate if there is no ...\n",
      "üìù Processing post 58/100: I Made Sure to be Repping in Animal Crossing...\n",
      "üìù Processing post 59/100: Does someone know the context to this pic?...\n",
      "üìù Processing post 58/100: I Made Sure to be Repping in Animal Crossing...\n",
      "üìù Processing post 59/100: Does someone know the context to this pic?...\n",
      "üìù Processing post 60/100: We have officially been ranked!...\n",
      "üìù Processing post 60/100: We have officially been ranked!...\n",
      "üìù Processing post 61/100: JMU upsets UVA, 52-49...\n",
      "üìù Processing post 62/100: JMU Men‚Äôs Basketball Sun Belt Conference Champions...\n",
      "üìù Processing post 61/100: JMU upsets UVA, 52-49...\n",
      "üìù Processing post 62/100: JMU Men‚Äôs Basketball Sun Belt Conference Champions...\n",
      "üìù Processing post 63/100: We‚Äôre screwed...\n",
      "üìù Processing post 63/100: We‚Äôre screwed...\n",
      "üìù Processing post 64/100: JMU 1-0!!!...\n",
      "üìù Processing post 65/100: Number 1 school for cases in VA. Roll dukes üòù...\n",
      "üìù Processing post 64/100: JMU 1-0!!!...\n",
      "üìù Processing post 65/100: Number 1 school for cases in VA. Roll dukes üòù...\n",
      "üìù Processing post 66/100: Eagle field last night. No social Distancing, Appr...\n",
      "üìù Processing post 66/100: Eagle field last night. No social Distancing, Appr...\n",
      "üìù Processing post 67/100: Virginia football in a nutshell...\n",
      "üìù Processing post 67/100: Virginia football in a nutshell...\n",
      "üìù Processing post 68/100: The spontaneous protests have returned!...\n",
      "üìù Processing post 68/100: The spontaneous protests have returned!...\n",
      "üìù Processing post 69/100: JMU offers CR/NC but you can‚Äôt use it for Major, M...\n",
      "üìù Processing post 69/100: JMU offers CR/NC but you can‚Äôt use it for Major, M...\n",
      "üìù Processing post 70/100: Our little quarantine project :)...\n",
      "üìù Processing post 71/100: My feelings exactly...\n",
      "üìù Processing post 70/100: Our little quarantine project :)...\n",
      "üìù Processing post 71/100: My feelings exactly...\n",
      "üìù Processing post 72/100: Anybody give a rundown of what happened at that hy...\n",
      "üìù Processing post 72/100: Anybody give a rundown of what happened at that hy...\n",
      "üìù Processing post 73/100: We lost a future Duke, y'all...\n",
      "üìù Processing post 73/100: We lost a future Duke, y'all...\n",
      "üìù Processing post 74/100: I feel this on a different level...\n",
      "üìù Processing post 74/100: I feel this on a different level...\n",
      "üìù Processing post 75/100: Pour one out for our injured canine...\n",
      "üìù Processing post 76/100: Flash mob rave at ECL in 2009...\n",
      "üìù Processing post 75/100: Pour one out for our injured canine...\n",
      "üìù Processing post 76/100: Flash mob rave at ECL in 2009...\n",
      "üìù Processing post 77/100: JMU can fuck right off with their \"Giving Week\" em...\n",
      "üìù Processing post 77/100: JMU can fuck right off with their \"Giving Week\" em...\n",
      "üìù Processing post 78/100: Just incase you need a good laugh...\n",
      "üìù Processing post 79/100: Me tonight in my pjs eating my Boorito bowl and st...\n",
      "üìù Processing post 78/100: Just incase you need a good laugh...\n",
      "üìù Processing post 79/100: Me tonight in my pjs eating my Boorito bowl and st...\n",
      "üìù Processing post 80/100: ‚ÄúWarm Regards‚Äù ~ Alger...\n",
      "üìù Processing post 81/100: Stay Safe - Don't go back...\n",
      "üìù Processing post 80/100: ‚ÄúWarm Regards‚Äù ~ Alger...\n",
      "üìù Processing post 81/100: Stay Safe - Don't go back...\n",
      "üìù Processing post 82/100: Well done!...\n",
      "üìù Processing post 82/100: Well done!...\n",
      "üìù Processing post 83/100: word of advice: don‚Äôt buy parking passes/meal plan...\n",
      "üìù Processing post 83/100: word of advice: don‚Äôt buy parking passes/meal plan...\n",
      "üìù Processing post 84/100: When I'm going to the library but I see the clubs ...\n",
      "üìù Processing post 85/100: Cursed cookout...\n",
      "üìù Processing post 84/100: When I'm going to the library but I see the clubs ...\n",
      "üìù Processing post 85/100: Cursed cookout...\n",
      "üìù Processing post 86/100: Last night, Ben DiNucci became the first-ever JMU ...\n",
      "üìù Processing post 87/100: Monitor someone put on the quad...\n",
      "üìù Processing post 86/100: Last night, Ben DiNucci became the first-ever JMU ...\n",
      "üìù Processing post 87/100: Monitor someone put on the quad...\n",
      "üìù Processing post 88/100: happy assessment day...\n",
      "üìù Processing post 89/100: Make good decisions tonight Dukes :)...\n",
      "üìù Processing post 88/100: happy assessment day...\n",
      "üìù Processing post 89/100: Make good decisions tonight Dukes :)...\n",
      "üìù Processing post 90/100: JMU Eliminates DEI...\n",
      "üìù Processing post 90/100: JMU Eliminates DEI...\n",
      "üìù Processing post 91/100: anyone know what's happening?...\n",
      "üìù Processing post 91/100: anyone know what's happening?...\n",
      "üìù Processing post 92/100: you guys fell for JMU's propaganda...\n",
      "üìù Processing post 92/100: you guys fell for JMU's propaganda...\n",
      "üìù Processing post 93/100: when you register for your last gen ed...\n",
      "üìù Processing post 94/100: JMU changes withdraw deadline to October 10th....\n",
      "üìù Processing post 93/100: when you register for your last gen ed...\n",
      "üìù Processing post 94/100: JMU changes withdraw deadline to October 10th....\n",
      "üìù Processing post 95/100: Happened right on the quad...\n",
      "üìù Processing post 95/100: Happened right on the quad...\n",
      "üìù Processing post 96/100: Photos of the Quad Cats in their new home!...\n",
      "üìù Processing post 97/100: Quad Cat Update!...\n",
      "üìù Processing post 96/100: Photos of the Quad Cats in their new home!...\n",
      "üìù Processing post 97/100: Quad Cat Update!...\n",
      "üìù Processing post 98/100: Be aware of this man‚Ä¶multiple people have already ...\n",
      "üìù Processing post 98/100: Be aware of this man‚Ä¶multiple people have already ...\n",
      "üìù Processing post 99/100: A bunch of photos that I took on Sunday, November ...\n",
      "üìù Processing post 100/100: If anyone is the Harrisonburg area needs food, ple...\n",
      "üìù Processing post 99/100: A bunch of photos that I took on Sunday, November ...\n",
      "üìù Processing post 100/100: If anyone is the Harrisonburg area needs food, ple...\n",
      "\n",
      "‚úÖ Successfully collected 1537 text items!\n",
      "üìä Ready for text analysis from r/JMU\n",
      "\n",
      "‚úÖ Successfully collected 1537 text items!\n",
      "üìä Ready for text analysis from r/JMU\n"
     ]
    }
   ],
   "source": [
    "\n",
    "subreddit_name = \"JMU\"  # Change this to any subreddit (without r/)\n",
    "num_posts = 100         # How many posts to scrape\n",
    "sort_method = \"top\"     # Options: \"hot\", \"new\", \"top\", \"rising\"\n",
    "\n",
    "print(f\"üéØ Scraping r/{subreddit_name} for text analysis\")\n",
    "print(f\"üìä Getting {num_posts} {sort_method} posts...\")\n",
    "print(\"‚è≥ This may take a minute...\")\n",
    "\n",
    "# Simple data structure - no duplicates\n",
    "text_data = []\n",
    "\n",
    "try:\n",
    "    # Get the subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    # Choose sorting method\n",
    "    if sort_method == \"hot\":\n",
    "        posts = subreddit.hot(limit=num_posts)\n",
    "    elif sort_method == \"new\":\n",
    "        posts = subreddit.new(limit=num_posts)\n",
    "    elif sort_method == \"top\":\n",
    "        posts = subreddit.top(limit=num_posts)\n",
    "    elif sort_method == \"rising\":\n",
    "        posts = subreddit.rising(limit=num_posts)\n",
    "    else:\n",
    "        posts = subreddit.hot(limit=num_posts)\n",
    "    \n",
    "    # Loop through each post\n",
    "    for post_num, submission in enumerate(posts, 1):\n",
    "        print(f\"üìù Processing post {post_num}/{num_posts}: {submission.title[:50]}...\")\n",
    "        \n",
    "        # Get post date for reference\n",
    "        post_date = datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Add the post itself (title + content)\n",
    "        post_text = submission.title\n",
    "        if submission.selftext.strip():  # Add post content if it exists\n",
    "            post_text += \" \" + submission.selftext\n",
    "            \n",
    "        text_data.append({\n",
    "            'type': 'post',\n",
    "            'title': submission.title,\n",
    "            'text': post_text,\n",
    "            'date': post_date,\n",
    "            'score': submission.score\n",
    "        })\n",
    "        \n",
    "        # Get comments for this post\n",
    "        try:\n",
    "            submission.comments.replace_more(limit=0)  # Don't expand \"more comments\"\n",
    "            \n",
    "            for comment in submission.comments.list()[:50]:  # Get more comments per post\n",
    "                if hasattr(comment, 'body') and comment.body.strip():\n",
    "                    comment_date = datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    \n",
    "                    text_data.append({\n",
    "                        'type': 'comment',\n",
    "                        'title': submission.title,  # Thread title for reference\n",
    "                        'text': comment.body,\n",
    "                        'date': comment_date,\n",
    "                        'score': comment.score\n",
    "                    })\n",
    "        except:\n",
    "            print(f\"   ‚ö†Ô∏è Could not load comments for this post\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Successfully collected {len(text_data)} text items!\")\n",
    "    print(f\"üìä Ready for text analysis from r/{subreddit_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error scraping subreddit: {e}\")\n",
    "    print(\"üí° Make sure the subreddit name is correct and accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab304ac",
   "metadata": {},
   "source": [
    "# Step 4: Generate Output\n",
    "\n",
    "Once the data has been created, you want to write that to a file. The script below creates two different files:\n",
    "\n",
    "- `reddit_text_analysis...csv`\n",
    "- `reddit_voyant.txt`\n",
    "\n",
    "We will use both files eventually. The `.csv` file is a table of all the comments and posts and the `.txt` file is the same information, but only as text, which is useful for visualizing in Voyant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18831ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà TEXT ANALYSIS STATS:\n",
      "   ‚Ä¢ Posts: 95\n",
      "   ‚Ä¢ Comments: 1285\n",
      "   ‚Ä¢ Total words: 41,568\n",
      "   ‚Ä¢ Average text length: 173 characters\n",
      "   ‚Ä¢ Longest text: 7651 characters\n",
      "\n",
      "üìã SAMPLE TEXTS:\n",
      "1. [POST] President Alger leaving to take same job at American University at the end of this academic year...\n",
      "2. [COMMENT] Like him or not, he did help transform this school. Applications to JMU have drastically increased u...\n",
      "3. [COMMENT] Massive changes happening at JMU this year. Alger stepping down, AD Bourne retiring, Cignetti left f...\n",
      "\n",
      "üéØ Created Voyant-ready file: data\\reddit_voyant_JMU_20250930_212751.txt\n"
     ]
    }
   ],
   "source": [
    "# üíæ Save Clean Text Data for Topic Modeling\n",
    "\n",
    "if text_data:\n",
    "    try:\n",
    "        # Create data folder if it doesn't exist\n",
    "        data_folder = \"data\"\n",
    "        os.makedirs(data_folder, exist_ok=True)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df_text = pd.DataFrame(text_data)\n",
    "        \n",
    "        # Clean text function for analysis\n",
    "        def clean_text_for_analysis(text):\n",
    "            if pd.isna(text):\n",
    "                return \"\"\n",
    "            text = str(text)\n",
    "            # Handle encoding\n",
    "            text = text.encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            # Remove control characters but keep newlines\n",
    "            text = ''.join(char for char in text if ord(char) >= 32 or char in '\\n\\r\\t')\n",
    "            # Remove extra whitespace\n",
    "            text = ' '.join(text.split())\n",
    "            return text\n",
    "        \n",
    "        # Clean the text data\n",
    "        df_text['text'] = df_text['text'].apply(clean_text_for_analysis)\n",
    "        df_text['title'] = df_text['title'].apply(clean_text_for_analysis)\n",
    "        \n",
    "        # Remove empty entries\n",
    "        df_text = df_text[df_text['text'].str.len() > 10]  # Remove very short texts\n",
    "        \n",
    "        # Ensure proper data types\n",
    "        df_text['score'] = pd.to_numeric(df_text['score'], errors='coerce')\n",
    "        df_text['date'] = pd.to_datetime(df_text['date'], errors='coerce')\n",
    "        df_text['type'] = df_text['type'].astype('category')\n",
    "        \n",
    "        # Create filename for analysis (save to data folder)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = os.path.join(data_folder, f\"reddit_text_analysis_{subreddit_name}_{sort_method}_num_posts_{num_posts}_{timestamp}.csv\")\n",
    "        \n",
    "        # Save to CSV optimized for text analysis\n",
    "        df_text.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "      \n",
    "          \n",
    "        \n",
    "        \n",
    "     \n",
    "        # Show statistics for text analysis\n",
    "        posts_count = (df_text['type'] == 'post').sum()\n",
    "        comments_count = (df_text['type'] == 'comment').sum()\n",
    "        \n",
    "        print(f\"\\nüìà TEXT ANALYSIS STATS:\")\n",
    "        print(f\"   ‚Ä¢ Posts: {posts_count}\")\n",
    "        print(f\"   ‚Ä¢ Comments: {comments_count}\")\n",
    "        print(f\"   ‚Ä¢ Total words: {df_text['text'].str.split().str.len().sum():,}\")\n",
    "        print(f\"   ‚Ä¢ Average text length: {df_text['text'].str.len().mean():.0f} characters\")\n",
    "        print(f\"   ‚Ä¢ Longest text: {df_text['text'].str.len().max()} characters\")\n",
    "        \n",
    "        print(f\"\\nüìã SAMPLE TEXTS:\")\n",
    "        for idx, row in enumerate(df_text.head(3).iterrows(), 1):\n",
    "            _, row_data = row\n",
    "            print(f\"{idx}. [{row_data['type'].upper()}] {row_data['text'][:100]}...\")\n",
    "        \n",
    "        # Create a separate file with just the text for Voyant (save to data folder)\n",
    "        text_only_filename = os.path.join(data_folder, f\"reddit_voyant_{subreddit_name}_{timestamp}.txt\")\n",
    "        with open(text_only_filename, 'w', encoding='utf-8') as f:\n",
    "            for _, row in df_text.iterrows():\n",
    "                f.write(f\"{row['text']}\\n\\n\")\n",
    "        \n",
    "        print(f\"\\nüéØ Created Voyant-ready file: {text_only_filename}\")\n",
    "        \n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving text data: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No text data to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3f8e15",
   "metadata": {},
   "source": [
    "## üéØ Simplified Text Scraper for Topic Modeling\n",
    "\n",
    "### üìù What This Does:\n",
    "This streamlined version focuses on **clean text extraction** for analysis tools like Voyant:\n",
    "- **No duplicates**: Each piece of text appears only once\n",
    "- **Clean format**: Text optimized for analysis\n",
    "- **Two outputs**: CSV for data analysis + TXT file for Voyant\n",
    "\n",
    "### \udd27 Quick Setup:\n",
    "1. **Change subreddit**: `subreddit_name = \"JMU\"`\n",
    "2. **Set post count**: `num_posts = 100` \n",
    "3. **Choose sorting**: `sort_method = \"hot\"`\n",
    "4. **Run both cells above**\n",
    "\n",
    "### \udcca What You Get:\n",
    "\n",
    "#### CSV File Contains:\n",
    "- **type**: \"post\" or \"comment\"\n",
    "- **title**: Thread title (for context)\n",
    "- **text**: The actual text content\n",
    "- **date**: When it was posted\n",
    "- **score**: Reddit score\n",
    "\n",
    "#### TXT File Contains:\n",
    "- Pure text, one item per line\n",
    "- Perfect for uploading directly to Voyant\n",
    "- No metadata, just content\n",
    "\n",
    "### üéØ Perfect for Topic Modeling:\n",
    "- **Voyant Tools**: Upload the .txt file directly\n",
    "- **Other text analysis**: Use the .csv file\n",
    "- **Clean data**: Removed duplicates and empty entries\n",
    "- **Focused content**: Just the text you need\n",
    "\n",
    "### üí° Analysis Tips:\n",
    "- **Voyant**: Use the .txt file for word clouds, trends, correlations\n",
    "- **Text length**: Longer texts work better for topic modeling\n",
    "- **Sample size**: 50-100 posts usually gives good results\n",
    "- **Time periods**: Try different sorting methods to see trends\n",
    "\n",
    "This approach gives you exactly what you need for text analysis without the complexity!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds101_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
