{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e1bde0-6884-482e-86f9-428755b47fdc",
   "metadata": {},
   "source": [
    "# Lesson 4.2: Geoparsing in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0cbc9a",
   "metadata": {},
   "source": [
    "## A big thank you!\n",
    "This lesson is made possible by the [geoparser](https://github.com/dguzh/geoparser) library created by Diego Gomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10701836-5eff-4b1b-8ee7-24f3d0859333",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lesson, we'll take text data we scraped from Reddit and:\n",
    "\n",
    "1. **Extract Locations**: Use a sophisticated geoparser to find and resolve geographic references\n",
    "3. **Visualize on Maps**: Create interactive maps showing retrieved locations\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dcb7c37-80eb-4ecf-91da-66d098d8825d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "try:\n",
    "    from geoparser import Geoparser\n",
    "    from tqdm.notebook import tqdm\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    import mapclassify as mc\n",
    "    import warnings\n",
    "    \n",
    "    # Suppress warnings for cleaner output\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    print(\"‚úÖ All libraries imported successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Missing library: {e}\")\n",
    "    print(\"Please run the installation notebook first: lesson_5_0_installation_setup.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e758d-0e34-4bb2-8ccc-cead9da5ebaf",
   "metadata": {},
   "source": [
    "## Part 1: Initialize the Geoparser System\n",
    "\n",
    "The geoparser is a sophisticated tool that can identify place names in text and resolve them to actual geographic coordinates. Here **resolve** means figure out the coordinates. The fancy word for this is actually **toponym disambiguation**. For example, we found Harrisonburg in our text, but we don't know necessarily which Harrisonburg. There is also a Harrisonburg in Louisiana. During toponym resolution the geoparser makes an educated guess as to which location is the right one. This can be based on a number of variables including the surrounding context, other places in the corpus, and even the weighting of the town size in a gazetteer. Harrisonburg, LA only has a few hundred residents, it is less likely someone is talking about that place.\n",
    "\n",
    "Google Maps also uses this technique to figure out what location you might be trying to find when you type it in on maps. Note that when you search for \"London\", London, UK is the first hit even though London,KY is closer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6ef4b-2414-4ed6-b360-692b44b061b5",
   "metadata": {},
   "source": [
    "### Step 1.1: Initialize the Geoparser\n",
    "\n",
    "We'll create a geoparser with optimized settings for accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5801c355-cc54-477b-8fe7-29f6e6b33de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing geoparser... (this may take a minute)\n",
      "‚ùå Error initializing geoparser: name 'Geoparser' is not defined\n",
      "Make sure you ran the installation notebook first!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(\"Initializing geoparser... (this may take a minute)\")\n",
    "    geo = Geoparser(\n",
    "        spacy_model='en_core_web_trf',                    # Advanced language model\n",
    "        transformer_model='dguzh/geo-all-distilroberta-v1', # Geographic transformer\n",
    "        gazetteer='geonames'                              # Geographic database\n",
    "    )\n",
    "    print(\"‚úÖ Geoparser initialized successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing geoparser: {e}\")\n",
    "    print(\"Make sure you ran the installation notebook first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f1ce5-1995-4dac-ac62-9073d34017ef",
   "metadata": {},
   "source": [
    "**What these parameters do:**\n",
    "- `spacy_model`: Advanced language processing for accurate text understanding\n",
    "- `transformer_model`: Specialized AI model trained to recognize geographic references  \n",
    "- `gazetteer`: Database containing millions of place names and their coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a23989-93da-46c4-abe5-2907d5c1aa28",
   "metadata": {},
   "source": [
    "### Step 1.2: Test the Geoparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f23571-3818-4d81-b63f-ed13fdd4096d",
   "metadata": {},
   "source": [
    "Let's test the geoparser with some sample sentences. Try changing the text below to include places you know:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de1b7b7-3399-462b-88de-0e8f1c04813f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toponym Recognition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 29.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toponym Resolution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:06<00:00,  9.60it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 66/66 [00:06<00:00,  9.60it/s]\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 14.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully parsed 3 sentences!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with sample sentences - feel free to modify these!\n",
    "test_sentences = [\n",
    "    \"I traveled from New York to Richmond, Virginia last summer.\",\n",
    "    \"The battle took place near Harrisonburg in the Shenandoah Valley.\",\n",
    "    \"London and Paris are popular European destinations.\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    docs = geo.parse(test_sentences)\n",
    "    print(f\"‚úÖ Successfully parsed {len(docs)} sentences!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during parsing: {e}\")\n",
    "    print(\"Try restarting the kernel and running from the beginning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca949f-1d2b-43bf-9725-e4f7d40f030d",
   "metadata": {},
   "source": [
    "### Step 1.3: Examine the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327daaf9-f0de-4de6-9c4a-b86781d27355",
   "metadata": {},
   "source": [
    "Let's see what locations the geoparser found. Each \"toponym\" is a place name with detailed geographic information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e03575a-c90c-46e3-a5e8-0691ae64ecec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó∫Ô∏è  LOCATIONS FOUND:\n",
      "==================================================\n",
      "\n",
      "Sentence 1: \"I traveled from New York to Richmond, Virginia last summer.\"\n",
      "  üìç Found: New York\n",
      "  üìç Found: Richmond\n",
      "  üìç Found: Virginia\n",
      "\n",
      "Sentence 2: \"The battle took place near Harrisonburg in the Shenandoah Valley.\"\n",
      "  üìç Found: Harrisonburg\n",
      "  üìç Found: the Shenandoah Valley\n",
      "\n",
      "Sentence 3: \"London and Paris are popular European destinations.\"\n",
      "  üìç Found: London\n",
      "  üìç Found: Paris\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"üó∫Ô∏è  LOCATIONS FOUND:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nSentence {i+1}: \\\"{test_sentences[i]}\\\"\")\n",
    "    \n",
    "    if doc.toponyms:\n",
    "        for toponym in doc.toponyms:\n",
    "            print(f\"  üìç Found: {toponym}\")\n",
    "    else:\n",
    "        print(\"  ‚ùå No locations found in this sentence\")\n",
    "        \n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dceb9cd-0f99-46e0-9135-f79660cb53e4",
   "metadata": {},
   "source": [
    "### Understanding the Data Structure\n",
    "\n",
    "Each toponym contains detailed geographic information. Here's what a complete location record looks like:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'geonameid': 2867714,\n",
    "    'name': 'Munich',\n",
    "    'latitude': 48.13743,\n",
    "    'longitude': 11.57549,\n",
    "    'country_name': 'Germany',\n",
    "    'admin1_name': 'Bavaria',        # State/Province\n",
    "    'admin2_name': 'Upper Bavaria',  # County/Region\n",
    "    'feature_name': 'seat of a first-order administrative division',\n",
    "    'population': 1260391\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02179074-c249-40a7-871d-97e453508517",
   "metadata": {},
   "source": [
    "We can access specific pieces of information using `.location['key_name']`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7291e59-eb5c-47c5-9a1f-7ac1d9c804c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìç DETAILED LOCATION DATA:\n",
      "============================================================\n",
      "\n",
      "Sentence 1: \"I traveled from New York to Richmond, Virginia last summer.\"\n",
      "  üèõÔ∏è  Place: New York\n",
      "  üåç Country: United States\n",
      "  üìê Coordinates: (43.0003, -75.4999)\n",
      "\n",
      "  üèõÔ∏è  Place: Richmond\n",
      "  üåç Country: United States\n",
      "  üìê Coordinates: (37.5538, -77.4603)\n",
      "\n",
      "  üèõÔ∏è  Place: Virginia\n",
      "  üåç Country: United States\n",
      "  üìê Coordinates: (37.5481, -77.4467)\n",
      "\n",
      "\n",
      "Sentence 2: \"The battle took place near Harrisonburg in the Shenandoah Valley.\"\n",
      "  üèõÔ∏è  Place: City of Harrisonburg\n",
      "  üåç Country: United States\n",
      "  üìê Coordinates: (38.4496, -78.8689)\n",
      "\n",
      "  üèõÔ∏è  Place: Community Christian School of the Shenandoah Valley\n",
      "  üåç Country: United States\n",
      "  üìê Coordinates: (38.9104, -78.4778)\n",
      "\n",
      "\n",
      "Sentence 3: \"London and Paris are popular European destinations.\"\n",
      "  üèõÔ∏è  Place: London\n",
      "  üåç Country: United Kingdom\n",
      "  üìê Coordinates: (51.5085, -0.1257)\n",
      "\n",
      "  üèõÔ∏è  Place: Paris\n",
      "  üåç Country: France\n",
      "  üìê Coordinates: (48.8534, 2.3488)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract specific geographic information\n",
    "print(\"üìç DETAILED LOCATION DATA:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\nSentence {i+1}: \\\"{test_sentences[i]}\\\"\")\n",
    "    \n",
    "    for toponym in doc.toponyms:\n",
    "        if toponym.location:\n",
    "            name = toponym.location['name']\n",
    "            lat = toponym.location['latitude']\n",
    "            lon = toponym.location['longitude']\n",
    "            country = toponym.location.get('country_name', 'Unknown')\n",
    "            \n",
    "            print(f\"  üèõÔ∏è  Place: {name}\")\n",
    "            print(f\"  üåç Country: {country}\")\n",
    "            print(f\"  üìê Coordinates: ({lat:.4f}, {lon:.4f})\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"  ‚ùå Location '{toponym}' could not be resolved to coordinates\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5a819-2928-4b91-a2b8-20aa858e5295",
   "metadata": {},
   "source": [
    "## Part 2: Load and Process JMU Text Data\n",
    "\n",
    "Now we'll want to extract locations from the Reddit data. We'll do so by loading the `.pickle` file from the previous lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7082c609-1d80-494a-9335-f269cf96de51",
   "metadata": {},
   "source": [
    "### Step 2.1: Load `JMU_reddit.pickle`\n",
    "\n",
    "This dataset contains sentences from the JMU reddit that have already been tagged for locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df00794-1c25-4f6e-9baa-a3c1617a5489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 30,005 sentences\n",
      "üìä Columns: ['type', 'date', 'score', 'year_month', 'sentences', 'toponyms']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df_jmu_reddit_toponyms = pd.read_pickle('data/jmu_reddit_toponyms.pickle')\n",
    "    print(f\"‚úÖ Loaded {len(df_jmu_reddit_toponyms):,} sentences\")\n",
    "   \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data file not found!\")\n",
    "    print(\"You may need to run previous lessons first to generate the sentiment data.\")\n",
    "    print(\"Or check that you're in the correct directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79209b12-06db-4fb7-8fc0-bd35d07cc5fd",
   "metadata": {},
   "source": [
    "### Step 2.2: The Geoparsing Function\n",
    "\n",
    "Here's a streamlined function that processes text and extracts geographic information:\n",
    "\n",
    "**Key features:**\n",
    "- Processes multiple sentences at once for efficiency\n",
    "- Extracts coordinates and place information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e7fef83-5a3a-4783-8b78-6e2b04d64338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoparse_dataframe(df, text_column='sentences'):\n",
    "    \"\"\"\n",
    "    Extract geographic locations from text data.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with text data\n",
    "        text_column: Column containing the text to parse\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with added location columns\n",
    "    \"\"\"\n",
    "    print(f\"üîç Processing {len(df)} sentences for geographic locations...\")\n",
    "    \n",
    "    # Convert text column to list for batch processing\n",
    "    sentences = df[text_column].tolist()\n",
    "    \n",
    "    try:\n",
    "        # Process all sentences at once (more efficient)\n",
    "        docs = geo.parse(sentences)  # Parse all sentences without feature filtering\n",
    "        \n",
    "        # Initialize storage for results\n",
    "        places, latitudes, longitudes, feature_names = [], [], [], []\n",
    "        \n",
    "        # Extract information from each processed document\n",
    "        for doc in tqdm(docs, desc=\"Extracting locations\"):\n",
    "            doc_places = []\n",
    "            doc_latitudes = []\n",
    "            doc_longitudes = []\n",
    "            doc_feature_names = []\n",
    "            \n",
    "            # Get all toponyms found in this document\n",
    "            for toponym in doc.toponyms:\n",
    "                if toponym.location:\n",
    "                    doc_places.append(toponym.location.get('name'))\n",
    "                    doc_latitudes.append(toponym.location.get('latitude'))\n",
    "                    doc_longitudes.append(toponym.location.get('longitude'))\n",
    "                    doc_feature_names.append(toponym.location.get('feature_name'))\n",
    "            \n",
    "            # Store results (empty lists if no locations found)\n",
    "            places.append(doc_places)\n",
    "            latitudes.append(doc_latitudes)\n",
    "            longitudes.append(doc_longitudes)\n",
    "            feature_names.append(doc_feature_names)\n",
    "        \n",
    "        # Add new columns to dataframe\n",
    "        df_result = df.copy()\n",
    "        df_result['place'] = places\n",
    "        df_result['latitude'] = latitudes\n",
    "        df_result['longitude'] = longitudes\n",
    "        df_result['feature_name'] = feature_names\n",
    "        \n",
    "        print(f\"‚úÖ Geoparsing complete!\")\n",
    "        return df_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during geoparsing: {e}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e802f-76ab-49eb-88dd-ab910b19b5f6",
   "metadata": {},
   "source": [
    "There are several interesting things of note in the data. First, for some of the sentences the tokenizer did not find a toponym which is indicated by empty lists `[]`. This because this is a more accurate tokenizer and will likely have fewer false positives. We will have to remember to remove these. \n",
    "\n",
    "Likewise, right now the parsing has been set to include Administrative areas like countries and states (i.e. The US and Virginia) and population centers (Richmond, Harrisonburg). We will have to think of how to deal with these down the road.\n",
    "\n",
    "**We can run the geoparser for all the data and expect to wait at least an hour!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a547180-0c60-4bf2-b0eb-22e9a2e8b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Processing 30005 sentences for geographic locations...\n",
      "Toponym Recognition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   2%|‚ñè         | 673/30005 [00:32<23:37, 20.69it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the geoparser over the entire 'sentences' column\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m geoparse_results = \u001b[43mgeoparse_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_jmu_reddit_toponyms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mgeoparse_dataframe\u001b[39m\u001b[34m(df, text_column)\u001b[39m\n\u001b[32m     15\u001b[39m sentences = df[text_column].tolist()\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Process all sentences at once (more efficient)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     docs = \u001b[43mgeo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Parse all sentences without feature filtering\u001b[39;00m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# Initialize storage for results\u001b[39;00m\n\u001b[32m     22\u001b[39m     places, latitudes, longitudes, feature_names = [], [], [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\geoparser\\geoparser\\geoparser.py:170\u001b[39m, in \u001b[36mGeoparser.parse\u001b[39m\u001b[34m(self, texts, batch_size, filter)\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput must be a list of strings\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mToponym Recognition...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecognize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mToponym Resolution...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m docs = \u001b[38;5;28mself\u001b[39m.resolve(docs, batch_size=batch_size, \u001b[38;5;28mfilter\u001b[39m=\u001b[38;5;28mfilter\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\geoparser\\geoparser\\geoparser.py:187\u001b[39m, in \u001b[36mGeoparser.recognize\u001b[39m\u001b[34m(self, texts, batch_size)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecognize\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: t.List[\u001b[38;5;28mstr\u001b[39m], batch_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m8\u001b[39m) -> t.List[GeoDoc]:\n\u001b[32m    177\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[33;03m    Perform toponym recognition on a list of texts.\u001b[39;00m\n\u001b[32m    179\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    185\u001b[39m \u001b[33;03m        List[GeoDoc]: List of GeoDoc objects with recognized toponyms.\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     docs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBatches\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1191\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1189\u001b[39m dt = cur_t - last_print_t\n\u001b[32m   1190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dt >= mininterval \u001b[38;5;129;01mand\u001b[39;00m cur_t >= min_start_t:\n\u001b[32m-> \u001b[39m\u001b[32m1191\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_print_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1192\u001b[39m     last_print_n = \u001b[38;5;28mself\u001b[39m.last_print_n\n\u001b[32m   1193\u001b[39m     last_print_t = \u001b[38;5;28mself\u001b[39m.last_print_t\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1242\u001b[39m, in \u001b[36mtqdm.update\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;28mself\u001b[39m._ema_dn(dn)\n\u001b[32m   1241\u001b[39m     \u001b[38;5;28mself\u001b[39m._ema_dt(dt)\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_args\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlock_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dynamic_miniters:\n\u001b[32m   1244\u001b[39m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[32m   1245\u001b[39m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[32m   1246\u001b[39m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[32m   1247\u001b[39m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maxinterval \u001b[38;5;129;01mand\u001b[39;00m dt >= \u001b[38;5;28mself\u001b[39m.maxinterval:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1347\u001b[39m, in \u001b[36mtqdm.refresh\u001b[39m\u001b[34m(self, nolock, lock_args)\u001b[39m\n\u001b[32m   1345\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1346\u001b[39m         \u001b[38;5;28mself\u001b[39m._lock.acquire()\n\u001b[32m-> \u001b[39m\u001b[32m1347\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[32m   1349\u001b[39m     \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:1495\u001b[39m, in \u001b[36mtqdm.display\u001b[39m\u001b[34m(self, msg, pos)\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;28mself\u001b[39m.moveto(pos)\n\u001b[32m-> \u001b[39m\u001b[32m1495\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1496\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[32m   1497\u001b[39m     \u001b[38;5;28mself\u001b[39m.moveto(-pos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:459\u001b[39m, in \u001b[36mtqdm.status_printer.<locals>.print_status\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprint_status\u001b[39m(s):\n\u001b[32m    458\u001b[39m     len_s = disp_len(s)\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     last_len[\u001b[32m0\u001b[39m] = len_s\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\std.py:453\u001b[39m, in \u001b[36mtqdm.status_printer.<locals>.fp_write\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfp_write\u001b[39m(s):\n\u001b[32m    452\u001b[39m     fp.write(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[32m--> \u001b[39m\u001b[32m453\u001b[39m     \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\utils.py:196\u001b[39m, in \u001b[36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args, **kwargs):\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m e.errno != \u001b[32m5\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\ipykernel\\iostream.py:609\u001b[39m, in \u001b[36mOutStream.flush\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    607\u001b[39m     \u001b[38;5;28mself\u001b[39m.pub_thread.schedule(evt.set)\n\u001b[32m    608\u001b[39m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    610\u001b[39m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[32m    611\u001b[39m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[32m    612\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIOStream.flush timed out\u001b[39m\u001b[33m\"\u001b[39m, file=sys.__stderr__)\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    657\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\joost\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the geoparser over the entire 'sentences' column\n",
    "geoparse_results = geoparse_dataframe(df_jmu_reddit_toponyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70a802-158f-4988-bf66-9b52aa048451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the updated DataFrame with new columns\n",
    "geoparse_results.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca69a7e-18f7-4543-8721-87454a0bf5a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'geoparse_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_reddit_geoparsed = \u001b[43mgeoparse_results\u001b[49m[geoparse_results[\u001b[33m'\u001b[39m\u001b[33mplace\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28mlen\u001b[39m) != \u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'geoparse_results' is not defined"
     ]
    }
   ],
   "source": [
    "df_reddit_geoparsed = geoparse_results[geoparse_results['place'].apply(len) != 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022afd6-59c8-4a67-baf4-a80604806e4e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_geoparsed.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59eb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_geoparsed_long = df_reddit_geoparsed.explode(['place', 'latitude', 'longitude', 'feature_name']).reset_index(drop=True)\n",
    "df_reddit_geoparsed_long.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b389da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_geoparsed_long.to_pickle('data/jmu_reddit_geoparsed_long.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812773e-e1cc-4249-a4de-6a6819707331",
   "metadata": {},
   "source": [
    "## Part 3: Visualizing Spatial Relations\n",
    "\n",
    "We can make a quick map of the results using `plotly` and `mapbox`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6066a5-23cd-480d-a71f-2f2309e70566",
   "metadata": {},
   "source": [
    "### Step 3.1: Load Pre-processed Data\n",
    "\n",
    "The complete geoparsing process took over an hour on a modern computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed71bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count each place while keeping latitude and longitude\n",
    "df_reddit_geoparsed_count = df_reddit_geoparsed_long.groupby('place').agg({\n",
    "    'latitude': 'first',    # Keep the first latitude for each place\n",
    "    'longitude': 'first',   # Keep the first longitude for each place\n",
    "    'place': 'size'         # Count the occurrences\n",
    "}).rename(columns={'place': 'count'}).reset_index()\n",
    "\n",
    "# Filter for places mentioned more than 4 times\n",
    "df_reddit_geoparsed_count = df_reddit_geoparsed_count[df_reddit_geoparsed_count['count'] > 4]\n",
    "df_reddit_geoparsed_count.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_map(\n",
    "    df_reddit_geoparsed_count,     # DataFrame with count data\n",
    "    lat=\"latitude\",               # Latitude column\n",
    "    lon=\"longitude\",              # Longitude column\n",
    "    hover_name=\"place\",           # Show place name on hover\n",
    "    size=\"count\",                 # Marker size based on count\n",
    "    hover_data={\n",
    "        \"count\": True,              # Show count in hover\n",
    "        \"latitude\": False,          # Hide coordinates in hover\n",
    "        \"longitude\": False\n",
    "    },\n",
    "    title=\"Geographic Locations Found in JMU Reddit Data\",\n",
    "    zoom=3,                       # Start with a broad view\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# Update the layout to use the default map style\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"open-street-map\",  # No token needed for this style\n",
    "    margin={\"r\":0,\"t\":50,\"l\":0,\"b\":0}  # Remove margins but keep space for title\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427a3238",
   "metadata": {},
   "source": [
    "#### Critical Questons\n",
    "\n",
    "- What does the map show about the locations? \n",
    "- Where does it do well? \n",
    "- Where are the mistakes?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds101_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
