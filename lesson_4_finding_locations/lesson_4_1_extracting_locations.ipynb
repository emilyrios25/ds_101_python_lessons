{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbedb195-26c6-447e-b34f-12e9bd7f1028",
   "metadata": {},
   "source": [
    "# Lesson 4: Extracting Toponyms in Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23590346-b175-4597-b968-5ff10b726f71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Overview\n",
    "\n",
    "This lesson will cover three sections:\n",
    "\n",
    "- Prepping the data\n",
    "- Running NER\n",
    "- Visualizing the data\n",
    "\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lesson we will learn how to extract place names from text. We will do so in two ways:\n",
    "\n",
    "- Using known place names\n",
    "- Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) teaches computers how to \"read\" and understand text like humans do. When we want to find place names in text, this becomes tricky because the same word can mean different things depending on how it's used.\n",
    "\n",
    "For example: \n",
    "\n",
    "- I visited **Washington** last summer and saw the Capitol building.\n",
    "- **Washington** led the Continental Army during the Revolutionary War.\n",
    "\n",
    "In the first sentence, \"Washington\" refers to Washington D.C. (a place). In the second sentence, \"Washington\" refers to George Washington (a person, not a place). Humans can easily tell the difference, but computers need help figuring this out.\n",
    "\n",
    "If we had a system where we simply gave computers a list of place names, we would get a lot of false positives. Therefore linguists started teaching computers the rules of grammar so they could understand **parts of speech**. By knowing *how* a word is being used, computers get more accurate in predicting what the word means in that context. \n",
    "\n",
    "By having a basic grammar it is easier to figure out what a word means:\n",
    "\n",
    "- The weather in London was cold and foggy.\n",
    "- Jack London wrote *Call of the Wild*.\n",
    "\n",
    "Here, a computer can figure out that the first \"London\" is a place (because we say \"in London\"), while \"Jack London\" is a person's name because it is performing the verb \"write\", something cities generally don't do.\n",
    "\n",
    "Over time, more sophisticated models have developed to help computers understand how language is being used. This process of finding locations, people, organizations, and other important information in text is called Named Entity Recognition (NER). It's very useful because it helps us understand what a text is really about, beyond just counting words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b6d017-4e35-4251-b0c0-a75a552eb27a",
   "metadata": {},
   "source": [
    "## 1 Prepping the data\n",
    "This unit will require `spacy` and `nltk` to be installed. Please see the preparation instructions before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb2f833-cb74-47b5-9e7e-50e7c992f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ce395a-29cd-4acc-8fd8-d4b78ce26d76",
   "metadata": {},
   "source": [
    "#### 1.1.1 Load the data\n",
    "The data folder should contain the `.pickle` file from last lesson with all of the type modifications we made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e96f12-5845-4681-b4c6-c8e0ed1e204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit = pd.read_pickle('data/jmu_reddit.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1803b7-d0f4-45b5-9a19-961eb00d0908",
   "metadata": {},
   "source": [
    "#### 1.1.2 Check the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b735bd8-2b5e-45b0-83a4-56fc63566f4c",
   "metadata": {},
   "source": [
    "Before doing anything it's always good to take a look at the data before processing. In previous instances we were using `.head()` to see the first 10 rows of the data frame. Now we'll use: `.sample()`. This provides a random sample. I have set the parameter to `5` meaning five rows. I have also included the `random_state` parameter. By setting this, I will get the exact same sample every time. This is for presentation purposes only. It just makes sure that your results and my results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a01454-4bd2-4201-b56e-cea5f2a94b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "type",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year_month",
         "rawType": "period[M]",
         "type": "unknown"
        }
       ],
       "ref": "d61fa607-a1c9-4ddd-a8ed-bd5e82194280",
       "rows": [
        [
         "2238",
         "comment",
         "Explosion?",
         "I saw it firsthand drove by it",
         "2020-10-17 17:11:22",
         "2",
         "2020-10"
        ],
        [
         "8929",
         "comment",
         "Frustrated Student",
         "Oh I absolutely agree. But these \"kids\" also needs to be shown what being compassionate and mature looks like. Can't do that when the adults let you get away with murder. I'm not saying \"because their brains aren't fully developed, it's not on them.\" Any person who is attending parties right now is a blessed idiot and I hope anyone caught throwing or attending parties gets world of shit. I'm ashamed to be an Alum right now, but I'm not surprised that this is happening. JMU historically is stupid lax on disciplinary action.",
         "2020-10-04 19:10:22",
         "3",
         "2020-10"
        ],
        [
         "6629",
         "comment",
         "JMU President",
         "Like donâ€™t get me wrong, I liked Alger. Granted Iâ€™ve never met Jim in person (heâ€™s only been president for 2 months) and I havenâ€™t been back to campus. Just wanted to make sure I wasnâ€™t the only one who noticed this",
         "2025-08-30 21:15:12",
         "2",
         "2025-08"
        ],
        [
         "412",
         "comment",
         "Consider: the Duke Dog with no eyebrows",
         "Homie is vibing hard",
         "2019-11-29 01:30:41",
         "12",
         "2019-11"
        ],
        [
         "3548",
         "comment",
         "JMU is going to go online and you all should prepare for the inevitable",
         "I agree, wholeheartedly. Many of my professors donâ€™t understand how to access their tools or use them. As far as I know, I have always had access to SPSS and ArcGIS at home (from my personal computer- not connected through JMU wifi). Those softwares have always been accessible. As for other softwares and programs, I have no idea",
         "2020-07-22 12:32:05",
         "3",
         "2020-07"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>comment</td>\n",
       "      <td>Explosion?</td>\n",
       "      <td>I saw it firsthand drove by it</td>\n",
       "      <td>2020-10-17 17:11:22</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8929</th>\n",
       "      <td>comment</td>\n",
       "      <td>Frustrated Student</td>\n",
       "      <td>Oh I absolutely agree. But these \"kids\" also n...</td>\n",
       "      <td>2020-10-04 19:10:22</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6629</th>\n",
       "      <td>comment</td>\n",
       "      <td>JMU President</td>\n",
       "      <td>Like donâ€™t get me wrong, I liked Alger. Grante...</td>\n",
       "      <td>2025-08-30 21:15:12</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>comment</td>\n",
       "      <td>Consider: the Duke Dog with no eyebrows</td>\n",
       "      <td>Homie is vibing hard</td>\n",
       "      <td>2019-11-29 01:30:41</td>\n",
       "      <td>12</td>\n",
       "      <td>2019-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3548</th>\n",
       "      <td>comment</td>\n",
       "      <td>JMU is going to go online and you all should p...</td>\n",
       "      <td>I agree, wholeheartedly. Many of my professors...</td>\n",
       "      <td>2020-07-22 12:32:05</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                                              title  \\\n",
       "2238  comment                                         Explosion?   \n",
       "8929  comment                                 Frustrated Student   \n",
       "6629  comment                                      JMU President   \n",
       "412   comment            Consider: the Duke Dog with no eyebrows   \n",
       "3548  comment  JMU is going to go online and you all should p...   \n",
       "\n",
       "                                                   text                date  \\\n",
       "2238                     I saw it firsthand drove by it 2020-10-17 17:11:22   \n",
       "8929  Oh I absolutely agree. But these \"kids\" also n... 2020-10-04 19:10:22   \n",
       "6629  Like donâ€™t get me wrong, I liked Alger. Grante... 2025-08-30 21:15:12   \n",
       "412                                Homie is vibing hard 2019-11-29 01:30:41   \n",
       "3548  I agree, wholeheartedly. Many of my professors... 2020-07-22 12:32:05   \n",
       "\n",
       "      score year_month  \n",
       "2238      2    2020-10  \n",
       "8929      3    2020-10  \n",
       "6629      2    2025-08  \n",
       "412      12    2019-11  \n",
       "3548      3    2020-07  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit.sample(n= 5, random_state= 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f1e08-8ba7-42b8-973e-3b42a4ac0a66",
   "metadata": {},
   "source": [
    "#### 1.1.3 Download the 'punkt' tokenizer from nltk \n",
    "This library helps us split text into sentences. It will download once and then we won't need it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed5e2e5c-3003-47b1-91f9-2d4407a5d291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\burgerjx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\burgerjx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ca2d9-0711-4dc0-85ea-5be08de890d5",
   "metadata": {},
   "source": [
    "## 2 Split `text` into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5d472-2944-4d45-b184-8c501e551f82",
   "metadata": {},
   "source": [
    "To make our lives a bit easier, we will first split each text into individual sentences. When we use the tokenizer to find all of the locations later on, the process will be quicker, because the tokenizer does not like very long strings. Also, since we know that we want to know the emotions of the sentences where toponyms occur, we are anticipating that way of organizing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44db6900-801d-46d9-9466-ef8490e86c05",
   "metadata": {},
   "source": [
    "The following line of code takes all of the texts and turns them into sentences. First, we `.apply` the function `nltk.sent_tokenize` function from the `nltk` (natural language toolkit) library we downloaded. This splits each text into its sentences based on punctuation, and puts them into a list in the column `sentences`. We then take that list and `explode` it. That sounds more dramatic than it really is! All we're doing is telling pandas to make a new row for each sentence in the list. It is easier to see what's going on by simply running the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a43277-04f4-4a2a-8d29-25a0ca8af0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split each text into individual sentences using NLTK\n",
    "df_with_sentence_lists = df_reddit.assign(sentences=df_reddit['text'].apply(nltk.sent_tokenize))\n",
    "\n",
    "# Step 2: Create a new row for each sentence (explode the lists)\n",
    "df_reddit_sentences = df_with_sentence_lists.explode('sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bab586-f306-4ef7-86bf-1f44e81a2e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "type",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "title",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "text",
         "rawType": "string",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year_month",
         "rawType": "period[M]",
         "type": "unknown"
        },
        {
         "name": "sentences",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "6f97cfe8-ba77-40fe-84e7-f12150767a36",
       "rows": [
        [
         "7515",
         "comment",
         "I was just accepted!",
         "Congratz, you're gonna love it. Pick the quad as your first choice for housing",
         "2014-01-09 17:09:40",
         "8",
         "2014-01",
         "Pick the quad as your first choice for housing"
        ],
        [
         "4435",
         "comment",
         "Missing Dog",
         "I don't know how? This dog is now safely home but would appreciate directions on posting pics for the future",
         "2020-09-16 11:32:26",
         "2",
         "2020-09",
         "This dog is now safely home but would appreciate directions on posting pics for the future"
        ],
        [
         "2906",
         "comment",
         "1 Year Ago, we thought it would only be 2 weeks tops...",
         "Don't remind me.",
         "2021-03-11 22:12:23",
         "7",
         "2021-03",
         "Don't remind me."
        ],
        [
         "8214",
         "comment",
         "Its so hard to find friends as an older JMU student",
         "My daughter went there as transfer student when she was 22. Same thing. She has always been well liked and popular... but all friend groups seemed to be formed and not many were welcoming. She ended up joining some \"be kind\" group (something like that) and did find a few friends.",
         "2025-02-09 21:58:25",
         "2",
         "2025-02",
         "She ended up joining some \"be kind\" group (something like that) and did find a few friends."
        ],
        [
         "1911",
         "comment",
         "President Alger Mishap!",
         "Holy shit dude I thought I was the only one! My freshman year, my suite mates and I though it would be funny to climb the DHub sign one night and take a picture. We started trying to climb up when we heard someone shout at us. We immediately thought it was the cops, but dear god it was so much worse. Standing between us and the exit was President Alger. He just stared at us in the darkness for a good 30 seconds, and none of us knew what the fuck to do. Then all of a sudden, the man spreads his arms out in a T-pose, and sprints straight towards us. This had to have been the fastest sprint Iâ€™ve ever seen in my life. He ran straight into our group, and with his outstretched arms, clotheslined every single one of us. We all fell to the ground, out cold. We woke up in the morning, barely able to remember what happened. I never knew if I dreamed it or not. Thank god you posted this, now I donâ€™t feel quite so alone.",
         "2019-08-27 21:36:40",
         "23",
         "2019-08",
         "He just stared at us in the darkness for a good 30 seconds, and none of us knew what the fuck to do."
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>year_month</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7515</th>\n",
       "      <td>comment</td>\n",
       "      <td>I was just accepted!</td>\n",
       "      <td>Congratz, you're gonna love it. Pick the quad ...</td>\n",
       "      <td>2014-01-09 17:09:40</td>\n",
       "      <td>8</td>\n",
       "      <td>2014-01</td>\n",
       "      <td>Pick the quad as your first choice for housing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4435</th>\n",
       "      <td>comment</td>\n",
       "      <td>Missing Dog</td>\n",
       "      <td>I don't know how? This dog is now safely home ...</td>\n",
       "      <td>2020-09-16 11:32:26</td>\n",
       "      <td>2</td>\n",
       "      <td>2020-09</td>\n",
       "      <td>This dog is now safely home but would apprecia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>comment</td>\n",
       "      <td>1 Year Ago, we thought it would only be 2 week...</td>\n",
       "      <td>Don't remind me.</td>\n",
       "      <td>2021-03-11 22:12:23</td>\n",
       "      <td>7</td>\n",
       "      <td>2021-03</td>\n",
       "      <td>Don't remind me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8214</th>\n",
       "      <td>comment</td>\n",
       "      <td>Its so hard to find friends as an older JMU st...</td>\n",
       "      <td>My daughter went there as transfer student whe...</td>\n",
       "      <td>2025-02-09 21:58:25</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-02</td>\n",
       "      <td>She ended up joining some \"be kind\" group (som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>comment</td>\n",
       "      <td>President Alger Mishap!</td>\n",
       "      <td>Holy shit dude I thought I was the only one! M...</td>\n",
       "      <td>2019-08-27 21:36:40</td>\n",
       "      <td>23</td>\n",
       "      <td>2019-08</td>\n",
       "      <td>He just stared at us in the darkness for a goo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         type                                              title  \\\n",
       "7515  comment                               I was just accepted!   \n",
       "4435  comment                                        Missing Dog   \n",
       "2906  comment  1 Year Ago, we thought it would only be 2 week...   \n",
       "8214  comment  Its so hard to find friends as an older JMU st...   \n",
       "1911  comment                            President Alger Mishap!   \n",
       "\n",
       "                                                   text                date  \\\n",
       "7515  Congratz, you're gonna love it. Pick the quad ... 2014-01-09 17:09:40   \n",
       "4435  I don't know how? This dog is now safely home ... 2020-09-16 11:32:26   \n",
       "2906                                   Don't remind me. 2021-03-11 22:12:23   \n",
       "8214  My daughter went there as transfer student whe... 2025-02-09 21:58:25   \n",
       "1911  Holy shit dude I thought I was the only one! M... 2019-08-27 21:36:40   \n",
       "\n",
       "      score year_month                                          sentences  \n",
       "7515      8    2014-01     Pick the quad as your first choice for housing  \n",
       "4435      2    2020-09  This dog is now safely home but would apprecia...  \n",
       "2906      7    2021-03                                   Don't remind me.  \n",
       "8214      2    2025-02  She ended up joining some \"be kind\" group (som...  \n",
       "1911     23    2019-08  He just stared at us in the darkness for a goo...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display a random sample of 5 sentences to see the results\n",
    "df_reddit_sentences.sample(n=5, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c586a9-2c07-4941-8bdf-2408fef4e8ea",
   "metadata": {},
   "source": [
    "### 2.1 Drop `text` and `title`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e09d22d-09d9-4277-9259-a9d0d069cf8b",
   "metadata": {},
   "source": [
    "Since we are essentially copying `text_data` over and over again it's a good practice to `.drop` it. We already have that information in the new sentences column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b302b4-983e-4930-a781-973330379316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_sentences = df_reddit_sentences.drop(columns=['text', 'title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46603b1-ae08-43ab-8b11-4834568bd07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "type",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year_month",
         "rawType": "period[M]",
         "type": "unknown"
        },
        {
         "name": "sentences",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "7bbbd9b0-825c-45c1-b6a5-df41b07b6007",
       "rows": [
        [
         "0",
         "post",
         "2024-03-18 12:47:10",
         "358",
         "2024-03",
         "President Alger leaving to take same job at American University at the end of this academic year"
        ],
        [
         "1",
         "comment",
         "2024-03-18 12:49:04",
         "82",
         "2024-03",
         "Like him or not, he did help transform this school."
        ],
        [
         "1",
         "comment",
         "2024-03-18 12:49:04",
         "82",
         "2024-03",
         "Applications to JMU have drastically increased under this leadership."
        ],
        [
         "2",
         "comment",
         "2024-03-18 12:50:05",
         "34",
         "2024-03",
         "Massive changes happening at JMU this year."
        ],
        [
         "2",
         "comment",
         "2024-03-18 12:50:05",
         "34",
         "2024-03",
         "Alger stepping down, AD Bourne retiring, Cignetti left for IU job, and rumor is Basketball Coach Byington is going to accept the WVU job"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>year_month</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>2024-03-18 12:47:10</td>\n",
       "      <td>358</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>President Alger leaving to take same job at Am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2024-03-18 12:49:04</td>\n",
       "      <td>82</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>Like him or not, he did help transform this sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>comment</td>\n",
       "      <td>2024-03-18 12:49:04</td>\n",
       "      <td>82</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>Applications to JMU have drastically increased...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>2024-03-18 12:50:05</td>\n",
       "      <td>34</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>Massive changes happening at JMU this year.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>comment</td>\n",
       "      <td>2024-03-18 12:50:05</td>\n",
       "      <td>34</td>\n",
       "      <td>2024-03</td>\n",
       "      <td>Alger stepping down, AD Bourne retiring, Cigne...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type                date  score year_month  \\\n",
       "0     post 2024-03-18 12:47:10    358    2024-03   \n",
       "1  comment 2024-03-18 12:49:04     82    2024-03   \n",
       "1  comment 2024-03-18 12:49:04     82    2024-03   \n",
       "2  comment 2024-03-18 12:50:05     34    2024-03   \n",
       "2  comment 2024-03-18 12:50:05     34    2024-03   \n",
       "\n",
       "                                           sentences  \n",
       "0  President Alger leaving to take same job at Am...  \n",
       "1  Like him or not, he did help transform this sc...  \n",
       "1  Applications to JMU have drastically increased...  \n",
       "2        Massive changes happening at JMU this year.  \n",
       "2  Alger stepping down, AD Bourne retiring, Cigne...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit_sentences.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce73c78-72de-43b1-8350-d8c44db722eb",
   "metadata": {},
   "source": [
    "## 3 Filtering by list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916f8d8-4750-48dc-9ff0-099ac6c802ed",
   "metadata": {},
   "source": [
    "The most basic way to figure out if the texts contain places is to go through each text and filter it with a list of known place names. For example, we can create a list of Virginia places:\n",
    "\n",
    "- 'Richmond'\n",
    "- 'Harrisonburg'\n",
    "- 'Lynchburg'\n",
    "- 'Roanoke'\n",
    "- 'Charlottesville'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37c1bc2-2f34-40c5-8ba5-b3b9d26e44f4",
   "metadata": {},
   "source": [
    "We can then use our `str.contains()` method to filter out any texts that contain the words above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0097a83d-2433-453a-9762-f228f3267999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Virginia cities we want to search for\n",
    "virginia_cities = ['Richmond', 'Harrisonburg', 'Lynchburg', 'Roanoke', 'Charlottesville']\n",
    "\n",
    "# Filter sentences that contain any of these city names\n",
    "# The '|' symbol means \"OR\" - so we're looking for sentences with ANY of these cities\n",
    "df_reddit_words = df_reddit_sentences[df_reddit_sentences.sentences.str.contains('|'.join(virginia_cities))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e4218bd-2bc1-4e13-8196-63d00bd5e239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentences",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5fea150d-4719-4cbf-a040-6f82bd79cb65",
       "rows": [
        [
         "6523",
         "Harrisonburg is a major hub for the underground economy."
        ],
        [
         "9773",
         "â€œ*Protecting the health of our Harrisonburg and Rockingham County communityâ€”including students, faculty, staffâ€”is our top priority, and we need to act swiftly to stop the spread as best we can."
        ],
        [
         "7852",
         "Harrisonburg honestly has a great variety of food."
        ],
        [
         "1973",
         "She had a major impact on the students who went to the school in Downtown Harrisonburg"
        ],
        [
         "4621",
         "Then, you could narrow it down to that individual that was in contact with them :) And your saying â€œwhen the rates continue to rise in Harrisonburgâ€ etc."
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "6523    Harrisonburg is a major hub for the undergroun...\n",
       "9773    â€œ*Protecting the health of our Harrisonburg an...\n",
       "7852    Harrisonburg honestly has a great variety of f...\n",
       "1973    She had a major impact on the students who wen...\n",
       "4621    Then, you could narrow it down to that individ...\n",
       "Name: sentences, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit_words['sentences'].sample(5, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e54be71-5cf9-492e-a8f7-1dcd606c3c41",
   "metadata": {},
   "source": [
    "## 4 Using Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6a72f5-637a-423c-baab-c056ee049798",
   "metadata": {},
   "source": [
    "Working with location names represents a unique computational problem. Since we do not know in advance what the names might be, and since some proper nouns can be both cities and people, (i.e. Jefferson, Washington, Lincoln, etc.) we need the computer to have some concept of what a place is. This is where language models come in. \n",
    "\n",
    "Language models have been trained on a lot of text and through statistical inference establish the different types of entities in a text (verb, noun, adjective etc.) and using this basic understanding of grammar, they can infer when something is being used as a location name and when something is being used as a person's name. \n",
    "\n",
    "One of the main libraries that Python uses to do this is `spacy`. The sample below goes through the basic procedure for extracting an entity. \n",
    "\n",
    "**Don't worry about how the code works, just look at the result**\n",
    "\n",
    "Notice how accurately it is able to distinguish between an organization in Virginia (UVA), a person Jefferson, and the place Jefferson (geopolitical entity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7872dfa-cd91-41bb-8047-c24b5547ba4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded fresh spaCy model: en_core_web_sm\n",
      "\n",
      "ðŸ“ Processing text:\n",
      "The University of Virginia is in the town of Charlottesville. \n",
      "          It was created by Jefferson. In the 1950s, William Faulkner gave a series of lectures there \n",
      "          about his fiction, most of which is set in Jefferson.\n",
      "\n",
      "ðŸ” Named entities found:\n",
      "  'The University of Virginia' -> ORG\n",
      "  'Charlottesville' -> GPE\n",
      "  'Jefferson' -> PERSON\n",
      "  'the 1950s' -> DATE\n",
      "  'William Faulkner' -> PERSON\n",
      "  'Jefferson' -> GPE\n",
      "\n",
      "ðŸ“š Entity Label Meanings:\n",
      "  ORG = Organization\n",
      "  GPE = Geopolitical Entity (places)\n",
      "  PERSON = Person\n",
      "  DATE = Date or time period\n"
     ]
    }
   ],
   "source": [
    "# Clear any cached spaCy models and reload fresh\n",
    "import sys\n",
    "if 'spacy' in sys.modules:\n",
    "    del sys.modules['spacy']\n",
    "if 'nlp' in locals():\n",
    "    del nlp\n",
    "\n",
    "import spacy\n",
    "import importlib.util\n",
    "import subprocess\n",
    "\n",
    "# Check if the model is installed\n",
    "model_name = \"en_core_web_sm\"\n",
    "if importlib.util.find_spec(model_name) is None:\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", model_name])\n",
    "\n",
    "# Load spaCy's English model fresh\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(f\"âœ… Loaded fresh spaCy model: {model_name}\")\n",
    "\n",
    "# The sentence to process - clear example text\n",
    "text = \"\"\"The University of Virginia is in the town of Charlottesville. \n",
    "          It was created by Jefferson. In the 1950s, William Faulkner gave a series of lectures there \n",
    "          about his fiction, most of which is set in Jefferson.\"\"\"\n",
    "\n",
    "print(f\"\\nðŸ“ Processing text:\\n{text}\")\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"\\nðŸ” Named entities found:\")\n",
    "# Extract and display named entities along with their labels\n",
    "for ent in doc.ents:\n",
    "    print(f\"  '{ent.text}' -> {ent.label_}\")\n",
    "    \n",
    "print(\"\\nðŸ“š Entity Label Meanings:\")\n",
    "print(\"  ORG = Organization\")\n",
    "print(\"  GPE = Geopolitical Entity (places)\")  \n",
    "print(\"  PERSON = Person\")\n",
    "print(\"  DATE = Date or time period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552f24a2-4533-41d6-9af2-62d475b8d30b",
   "metadata": {},
   "source": [
    "spaCy is only as accurate as the data provided to it. If the text data is garbled or too short, it will likely have trouble. Undoubtedly, there are sentences in our `sentences` column that are not going to be read properly, but what we are relying on is the sheer volume of text. Even with some false positives and false negatives, we should be able to build a pretty good overview of the most mentioned places."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf1e07-2943-4245-a203-91d95b58cc95",
   "metadata": {},
   "source": [
    "## 5 Extract Entities in all `sentences`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aeeb45-0c18-4166-b34b-076f77bcb0d2",
   "metadata": {},
   "source": [
    "Doing one extraction on one sentence in `spacy` is pretty straight forward. We simply run the function `nlp()` on whatever sentence we want to analyze and save the result to a new variable, usually called `doc`. When we run this on a column with thousands of sentences, you start to run into performance issues because you are doing the procedure one at a time, and you also don't really know what's going on because there's no feedback. The functions below modify the above procedure a bit and basically asks your computer to use multiple processors, and it also provides a little progress bar. Finally, instead of using the very small model that we used above, we are now going to use a slightly bigger model called `en_core_web_md`, this will hopefully help us find more locations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24d05da0-ed0f-4a40-b744-81f10239ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb176f4f-76ae-4be8-9cad-cb28fe58fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded fresh spaCy model: en_core_web_md\n"
     ]
    }
   ],
   "source": [
    "# Clear any cached spaCy models and reload fresh\n",
    "import sys\n",
    "if 'spacy' in sys.modules:\n",
    "    del sys.modules['spacy']\n",
    "if 'nlp' in locals():\n",
    "    del nlp\n",
    "\n",
    "import spacy\n",
    "import importlib.util\n",
    "import subprocess\n",
    "\n",
    "# Check if the model is installed\n",
    "model_name = \"en_core_web_md\"\n",
    "if importlib.util.find_spec(model_name) is None:\n",
    "    subprocess.run([\"python\", \"-m\", \"spacy\", \"download\", model_name])\n",
    "\n",
    "# Load spaCy's English model fresh\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "print(f\"âœ… Loaded fresh spaCy model: {model_name}\")\n",
    "\n",
    "\n",
    "# Function to extract GPE (Geopolitical Entities) from a batch of docs\n",
    "def extract_gpe_from_docs(docs):\n",
    "    return [[ent.text for ent in doc.ents if ent.label_ == 'GPE'] or None for doc in docs]\n",
    "\n",
    "# Use nlp.pipe() for faster batch processing with multiple cores\n",
    "def process_sentences_in_batches(sentences, batch_size=50, n_process=-1):\n",
    "    # Process sentences using nlp.pipe with batch processing and multi-processing\n",
    "    gpe_results = []\n",
    "    for doc in tqdm(nlp.pipe(sentences, batch_size=batch_size, n_process=n_process), total=len(sentences)):\n",
    "        gpes = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        gpe_results.append(gpes if gpes else None)\n",
    "    return gpe_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94516a9b-ca65-4d0b-90ed-f71c48199445",
   "metadata": {},
   "source": [
    "We are now ready to apply this function to `sentences` and create a new column called `toponyms`. \n",
    "\n",
    "**Warning this process will take a couple of minutes**\n",
    "\n",
    "If this does not work, I have saved the results as `jmu_reddit_toponyms.pickle` and preloaded it in your data folder. You can simply keep running the code below on that imported file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9a92de05-4234-4d9c-bb0c-9ea9179466c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30005/30005 [01:59<00:00, 250.70it/s] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the function with tqdm progress bar and nlp.pipe() for batch processing\n",
    "df_reddit_sentences['toponyms'] = process_sentences_in_batches(df_reddit_sentences['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f9d634f-f41b-4bc9-aa32-618403ae2866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "sentences",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "toponyms",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "1db50f30-7668-4e1e-ac86-5221cf29dc8e",
       "rows": [
        [
         "3919",
         "Breonna Taylor's boyfriend shot at the police",
         null
        ],
        [
         "183",
         "We Re-Opened Campus!",
         null
        ],
        [
         "2466",
         "Iâ€™m not surprised they handled it this way, but I am disappointed in the administration.",
         null
        ],
        [
         "9675",
         "That or walk to Sheetz.",
         "['Sheetz']"
        ],
        [
         "7196",
         "I agree completely!",
         null
        ],
        [
         "5319",
         "!",
         null
        ],
        [
         "7030",
         "I went to my general doctor, told her I think I have ADHD and she referred me to a psychologist who had me do a ton of tests.",
         null
        ],
        [
         "10716",
         "I have no idea if it's good or bad",
         null
        ],
        [
         "5028",
         "Next to be received is my high school one",
         null
        ],
        [
         "8912",
         "Then go ahead and join the server",
         null
        ],
        [
         "7019",
         "Make no mistake though, you are probably mentally and emotionally struggling more than most.",
         null
        ],
        [
         "3337",
         "I had a massive knowledge dump since graduation lol",
         null
        ],
        [
         "6165",
         "Today's excel update is out: https://www.jmu.edu/stop-the-spread/_files/jmu-daily-case-update.xlsx",
         null
        ],
        [
         "9281",
         "The buses are really nice and come frequently, but unless things have changed, there are no direct routes to Walmart/the mall so that was annoying for me since I didnâ€™t have a car (but food lion is just down the hill so itâ€™s not a bad walk if youâ€™re not buying too much).",
         null
        ],
        [
         "5008",
         "Hello STRESSED, I'm dad What would seem to be the problem?",
         null
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 15
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>toponyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3919</th>\n",
       "      <td>Breonna Taylor's boyfriend shot at the police</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>We Re-Opened Campus!</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>Iâ€™m not surprised they handled it this way, bu...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9675</th>\n",
       "      <td>That or walk to Sheetz.</td>\n",
       "      <td>[Sheetz]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7196</th>\n",
       "      <td>I agree completely!</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5319</th>\n",
       "      <td>!</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7030</th>\n",
       "      <td>I went to my general doctor, told her I think ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10716</th>\n",
       "      <td>I have no idea if it's good or bad</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5028</th>\n",
       "      <td>Next to be received is my high school one</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912</th>\n",
       "      <td>Then go ahead and join the server</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7019</th>\n",
       "      <td>Make no mistake though, you are probably menta...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3337</th>\n",
       "      <td>I had a massive knowledge dump since graduatio...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6165</th>\n",
       "      <td>Today's excel update is out: https://www.jmu.e...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9281</th>\n",
       "      <td>The buses are really nice and come frequently,...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>Hello STRESSED, I'm dad What would seem to be ...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentences  toponyms\n",
       "3919       Breonna Taylor's boyfriend shot at the police      None\n",
       "183                                 We Re-Opened Campus!      None\n",
       "2466   Iâ€™m not surprised they handled it this way, bu...      None\n",
       "9675                             That or walk to Sheetz.  [Sheetz]\n",
       "7196                                 I agree completely!      None\n",
       "5319                                                   !      None\n",
       "7030   I went to my general doctor, told her I think ...      None\n",
       "10716                 I have no idea if it's good or bad      None\n",
       "5028           Next to be received is my high school one      None\n",
       "8912                   Then go ahead and join the server      None\n",
       "7019   Make no mistake though, you are probably menta...      None\n",
       "3337   I had a massive knowledge dump since graduatio...      None\n",
       "6165   Today's excel update is out: https://www.jmu.e...      None\n",
       "9281   The buses are really nice and come frequently,...      None\n",
       "5008   Hello STRESSED, I'm dad What would seem to be ...      None"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the result\n",
    "df_reddit_sentences[['sentences', 'toponyms']].sample(15, random_state = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cfa606-a7ec-4a2b-82f8-79a329e81109",
   "metadata": {},
   "source": [
    "Because not every sentence includes a toponym sometimes it will say `Missing value`. We will want to eliminate all the rows with none in them, because they are not relevant and they take up memory. Still, we might want to peek inside and calculate what percentage of sentences actually have toponyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e524ddf5-dc3e-4add-817c-7da1d4ea4ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of sentences with toponyms: 4.06%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of sentences with toponyms (not None or empty)\n",
    "num_sentences_with_toponyms = df_reddit_sentences['toponyms'].dropna().apply(lambda x: len(x) > 0).sum()\n",
    "\n",
    "# Calculate the total number of sentences\n",
    "total_sentences = len(df_reddit_sentences)\n",
    "\n",
    "# Calculate the percentage of sentences with toponyms\n",
    "percent_with_toponyms = (num_sentences_with_toponyms / total_sentences) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f\"Percentage of sentences with toponyms: {percent_with_toponyms:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547ced0-776c-4673-846b-71ec2f66fcbe",
   "metadata": {},
   "source": [
    "The total is pretty low when we compare it to the Hauser text, but keep in mind there may still be false positives here. Let's clear out all of the rows without toponyms. We can use a filter to get all of the rows that have data in the `toponyms` column `.notna()`, and also to check to make sure that if there is a list, that the list actually contains values `str.len()>0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "476cb0ef-83b3-4dda-a83a-6333f19f81d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "type",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "score",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "year_month",
         "rawType": "period[M]",
         "type": "unknown"
        },
        {
         "name": "sentences",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "toponyms",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "69a7da88-c221-43aa-aa2b-f4ac530e3714",
       "rows": [
        [
         "9015",
         "post",
         "2018-11-03 00:20:12",
         "36",
         "2018-11",
         "Hey JMU, if you're registered to vote in VA, but not in Harrisonburg, TODAY (saturday) is the LAST day to vote absentee in your home town.",
         "['VA', 'Harrisonburg']"
        ],
        [
         "10127",
         "post",
         "2021-09-06 10:45:22",
         "32",
         "2021-09",
         "anyone know where they moved the condom rack thing there used to be a huge shelf of free condoms in SSC but it moved and I havent seen it UPDATE: its in the wellness center now located 2nd floor of UREC",
         "['UREC']"
        ],
        [
         "763",
         "comment",
         "2020-08-13 10:47:20",
         "-37",
         "2020-08",
         "Just remember this as you grow into adults that run this country: America as a whole operates under the assumption that money is more important than the lives of the population.",
         "['America']"
        ],
        [
         "6307",
         "post",
         "2024-09-25 08:12:43",
         "48",
         "2024-09",
         "I read the owner of Daveâ€™s now owns Francescoâ€™s in Bridgewater but Iâ€™m looking for a place in Harrisonburg.",
         "['Bridgewater', 'Harrisonburg']"
        ],
        [
         "9442",
         "comment",
         "2021-04-28 11:36:37",
         "23",
         "2021-04",
         "Yeah my place is managed by forrest hills and it's pathetic.",
         "['forrest hills']"
        ],
        [
         "3988",
         "comment",
         "2020-09-01 11:23:52",
         "17",
         "2020-09",
         "In this area of Virginia, per the VDH dashboard, 100% of COVID deaths have been 50+.",
         "['Virginia']"
        ],
        [
         "6838",
         "comment",
         "2022-10-16 08:43:06",
         "10",
         "2022-10",
         "If theyâ€™re getting airlifted to Charlottesville then theyâ€™re most likely in critical condition.",
         "['Charlottesville']"
        ],
        [
         "9013",
         "comment",
         "2020-06-06 17:59:30",
         "1",
         "2020-06",
         "However, nearly everyone in the state of VA knows about James Madison.",
         "['VA']"
        ],
        [
         "8640",
         "comment",
         "2021-05-20 21:43:29",
         "2",
         "2021-05",
         "Once new shrubs are put in front of Burruss, the cooler shelter and the feeding station will (I think) be moved back to where they were.",
         "['Burruss']"
        ],
        [
         "1602",
         "comment",
         "2020-08-27 12:39:26",
         "98",
         "2020-08",
         "JMU knowingly and actively endangered the health of students and the Harrisonburg community because money is more important than lives to JMU.",
         "['Harrisonburg']"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>date</th>\n",
       "      <th>score</th>\n",
       "      <th>year_month</th>\n",
       "      <th>sentences</th>\n",
       "      <th>toponyms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9015</th>\n",
       "      <td>post</td>\n",
       "      <td>2018-11-03 00:20:12</td>\n",
       "      <td>36</td>\n",
       "      <td>2018-11</td>\n",
       "      <td>Hey JMU, if you're registered to vote in VA, b...</td>\n",
       "      <td>[VA, Harrisonburg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10127</th>\n",
       "      <td>post</td>\n",
       "      <td>2021-09-06 10:45:22</td>\n",
       "      <td>32</td>\n",
       "      <td>2021-09</td>\n",
       "      <td>anyone know where they moved the condom rack t...</td>\n",
       "      <td>[UREC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>comment</td>\n",
       "      <td>2020-08-13 10:47:20</td>\n",
       "      <td>-37</td>\n",
       "      <td>2020-08</td>\n",
       "      <td>Just remember this as you grow into adults tha...</td>\n",
       "      <td>[America]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6307</th>\n",
       "      <td>post</td>\n",
       "      <td>2024-09-25 08:12:43</td>\n",
       "      <td>48</td>\n",
       "      <td>2024-09</td>\n",
       "      <td>I read the owner of Daveâ€™s now owns Francescoâ€™...</td>\n",
       "      <td>[Bridgewater, Harrisonburg]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9442</th>\n",
       "      <td>comment</td>\n",
       "      <td>2021-04-28 11:36:37</td>\n",
       "      <td>23</td>\n",
       "      <td>2021-04</td>\n",
       "      <td>Yeah my place is managed by forrest hills and ...</td>\n",
       "      <td>[forrest hills]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988</th>\n",
       "      <td>comment</td>\n",
       "      <td>2020-09-01 11:23:52</td>\n",
       "      <td>17</td>\n",
       "      <td>2020-09</td>\n",
       "      <td>In this area of Virginia, per the VDH dashboar...</td>\n",
       "      <td>[Virginia]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6838</th>\n",
       "      <td>comment</td>\n",
       "      <td>2022-10-16 08:43:06</td>\n",
       "      <td>10</td>\n",
       "      <td>2022-10</td>\n",
       "      <td>If theyâ€™re getting airlifted to Charlottesvill...</td>\n",
       "      <td>[Charlottesville]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9013</th>\n",
       "      <td>comment</td>\n",
       "      <td>2020-06-06 17:59:30</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-06</td>\n",
       "      <td>However, nearly everyone in the state of VA kn...</td>\n",
       "      <td>[VA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8640</th>\n",
       "      <td>comment</td>\n",
       "      <td>2021-05-20 21:43:29</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-05</td>\n",
       "      <td>Once new shrubs are put in front of Burruss, t...</td>\n",
       "      <td>[Burruss]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>comment</td>\n",
       "      <td>2020-08-27 12:39:26</td>\n",
       "      <td>98</td>\n",
       "      <td>2020-08</td>\n",
       "      <td>JMU knowingly and actively endangered the heal...</td>\n",
       "      <td>[Harrisonburg]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                date  score year_month  \\\n",
       "9015      post 2018-11-03 00:20:12     36    2018-11   \n",
       "10127     post 2021-09-06 10:45:22     32    2021-09   \n",
       "763    comment 2020-08-13 10:47:20    -37    2020-08   \n",
       "6307      post 2024-09-25 08:12:43     48    2024-09   \n",
       "9442   comment 2021-04-28 11:36:37     23    2021-04   \n",
       "3988   comment 2020-09-01 11:23:52     17    2020-09   \n",
       "6838   comment 2022-10-16 08:43:06     10    2022-10   \n",
       "9013   comment 2020-06-06 17:59:30      1    2020-06   \n",
       "8640   comment 2021-05-20 21:43:29      2    2021-05   \n",
       "1602   comment 2020-08-27 12:39:26     98    2020-08   \n",
       "\n",
       "                                               sentences  \\\n",
       "9015   Hey JMU, if you're registered to vote in VA, b...   \n",
       "10127  anyone know where they moved the condom rack t...   \n",
       "763    Just remember this as you grow into adults tha...   \n",
       "6307   I read the owner of Daveâ€™s now owns Francescoâ€™...   \n",
       "9442   Yeah my place is managed by forrest hills and ...   \n",
       "3988   In this area of Virginia, per the VDH dashboar...   \n",
       "6838   If theyâ€™re getting airlifted to Charlottesvill...   \n",
       "9013   However, nearly everyone in the state of VA kn...   \n",
       "8640   Once new shrubs are put in front of Burruss, t...   \n",
       "1602   JMU knowingly and actively endangered the heal...   \n",
       "\n",
       "                          toponyms  \n",
       "9015            [VA, Harrisonburg]  \n",
       "10127                       [UREC]  \n",
       "763                      [America]  \n",
       "6307   [Bridgewater, Harrisonburg]  \n",
       "9442               [forrest hills]  \n",
       "3988                    [Virginia]  \n",
       "6838             [Charlottesville]  \n",
       "9013                          [VA]  \n",
       "8640                     [Burruss]  \n",
       "1602                [Harrisonburg]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit_toponyms = df_reddit_sentences[df_reddit_sentences['toponyms'].notna() & df_reddit_sentences['toponyms'].str.len() > 0]\n",
    "\n",
    "# Display the first few rows of the new DataFrame\n",
    "df_reddit_toponyms.sample(10, random_state = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55041e7a-bfdf-4ffc-b317-d0c137bb62a5",
   "metadata": {},
   "source": [
    "## 6 Counting Toponyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5eb055-7678-43a0-bc23-6e1d3bd4a623",
   "metadata": {},
   "source": [
    "We can gain more insights into the data if we visualize them. Since this takes some complicated coding, don't worry about how this result is achieved for now. Suffice to say that what the code does is count the total number of each toponym in the corpus to get the raw value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb4ebc82-65a8-415a-81bf-8ebf1aa023fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e72624ff-4ca0-4508-9594-c95121e22dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Unnest the list (flatten the 'toponyms' column)\n",
    "unnested_toponyms = df_reddit_toponyms['toponyms'].explode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93e5df2c-2b8e-4dfd-ba1c-d46f1f7976e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Drop any NaN values (if any exist)\n",
    "unnested_toponyms = unnested_toponyms.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5e54a18d-9c77-48d4-93f8-ec4e5bb664d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Collapse the unnested toponyms by count\n",
    "toponym_counts = Counter(unnested_toponyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c3693a2-4acf-4b5b-8f4e-b2cd03a5a770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Toponym",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "faf9c093-de2b-4380-9302-c08be243a4b4",
       "rows": [
        [
         "4",
         "Harrisonburg",
         "240"
        ],
        [
         "7",
         "Virginia",
         "114"
        ],
        [
         "8",
         "VT",
         "47"
        ],
        [
         "40",
         "VA",
         "42"
        ],
        [
         "19",
         "US",
         "41"
        ],
        [
         "72",
         "Florida",
         "21"
        ],
        [
         "66",
         "harrisonburg",
         "20"
        ],
        [
         "23",
         "America",
         "19"
        ],
        [
         "65",
         "Breeze",
         "19"
        ],
        [
         "47",
         "FCS",
         "19"
        ],
        [
         "49",
         "Jersey",
         "17"
        ],
        [
         "16",
         "Covid",
         "16"
        ],
        [
         "71",
         "Richmond",
         "15"
        ],
        [
         "61",
         "SMAD",
         "14"
        ],
        [
         "1",
         "Burruss",
         "13"
        ],
        [
         "5",
         "DC",
         "10"
        ],
        [
         "42",
         "Charlottesville",
         "10"
        ],
        [
         "237",
         "Aspen",
         "10"
        ],
        [
         "129",
         "NJ",
         "9"
        ],
        [
         "69",
         "Bridgewater",
         "9"
        ],
        [
         "102",
         "NY",
         "9"
        ],
        [
         "28",
         "U.S.",
         "8"
        ],
        [
         "24",
         "Rockingham County",
         "8"
        ],
        [
         "183",
         "Hburg",
         "8"
        ],
        [
         "362",
         "Salem",
         "8"
        ],
        [
         "363",
         "Lynchburg",
         "7"
        ],
        [
         "118",
         "the United States",
         "7"
        ],
        [
         "51",
         "West Virginia",
         "6"
        ],
        [
         "35",
         "Madison",
         "6"
        ],
        [
         "36",
         "Wayland",
         "6"
        ],
        [
         "20",
         "P.S.",
         "6"
        ],
        [
         "146",
         "Godwin",
         "6"
        ],
        [
         "18",
         "virginia",
         "6"
        ],
        [
         "133",
         "VIRGINIA",
         "6"
        ],
        [
         "110",
         "Maury",
         "6"
        ],
        [
         "84",
         "New Zealand",
         "6"
        ],
        [
         "121",
         "North Carolina",
         "5"
        ],
        [
         "187",
         "New Jersey",
         "5"
        ],
        [
         "82",
         "Chestnut Ridge",
         "5"
        ],
        [
         "199",
         "New York",
         "5"
        ],
        [
         "22",
         "BS",
         "5"
        ],
        [
         "343",
         "Lakeside",
         "5"
        ],
        [
         "349",
         "Dayton",
         "4"
        ],
        [
         "269",
         "Indiana",
         "4"
        ],
        [
         "103",
         "App State",
         "4"
        ],
        [
         "112",
         "Showker",
         "4"
        ],
        [
         "14",
         "Wisconsin",
         "4"
        ],
        [
         "2",
         "Washington",
         "4"
        ],
        [
         "188",
         "mymadison",
         "4"
        ],
        [
         "195",
         "jersey",
         "4"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 50
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Toponym</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harrisonburg</td>\n",
       "      <td>240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VT</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>VA</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>US</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Florida</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>harrisonburg</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>America</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Breeze</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>FCS</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Jersey</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Covid</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Richmond</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>SMAD</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Burruss</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DC</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Charlottesville</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Aspen</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>NJ</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Bridgewater</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>NY</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>U.S.</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Rockingham County</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Hburg</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Salem</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Lynchburg</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>the United States</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Madison</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Wayland</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>P.S.</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Godwin</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>virginia</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Maury</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Chestnut Ridge</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>New York</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>BS</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Lakeside</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Dayton</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>App State</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Showker</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Washington</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>mymadison</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>jersey</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Toponym  Count\n",
       "4         Harrisonburg    240\n",
       "7             Virginia    114\n",
       "8                   VT     47\n",
       "40                  VA     42\n",
       "19                  US     41\n",
       "72             Florida     21\n",
       "66        harrisonburg     20\n",
       "23             America     19\n",
       "65              Breeze     19\n",
       "47                 FCS     19\n",
       "49              Jersey     17\n",
       "16               Covid     16\n",
       "71            Richmond     15\n",
       "61                SMAD     14\n",
       "1              Burruss     13\n",
       "5                   DC     10\n",
       "42     Charlottesville     10\n",
       "237              Aspen     10\n",
       "129                 NJ      9\n",
       "69         Bridgewater      9\n",
       "102                 NY      9\n",
       "28                U.S.      8\n",
       "24   Rockingham County      8\n",
       "183              Hburg      8\n",
       "362              Salem      8\n",
       "363          Lynchburg      7\n",
       "118  the United States      7\n",
       "51       West Virginia      6\n",
       "35             Madison      6\n",
       "36             Wayland      6\n",
       "20                P.S.      6\n",
       "146             Godwin      6\n",
       "18            virginia      6\n",
       "133           VIRGINIA      6\n",
       "110              Maury      6\n",
       "84         New Zealand      6\n",
       "121     North Carolina      5\n",
       "187         New Jersey      5\n",
       "82      Chestnut Ridge      5\n",
       "199           New York      5\n",
       "22                  BS      5\n",
       "343           Lakeside      5\n",
       "349             Dayton      4\n",
       "269            Indiana      4\n",
       "103          App State      4\n",
       "112            Showker      4\n",
       "14           Wisconsin      4\n",
       "2           Washington      4\n",
       "188          mymadison      4\n",
       "195             jersey      4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to a DataFrame for easy viewing\n",
    "toponym_counts_df = pd.DataFrame(toponym_counts.items(), columns=['Toponym', 'Count']).sort_values(by='Count', ascending=False)\n",
    "toponym_counts_df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcbb1ac-ce59-4108-9adf-31a0d2eeb6f7",
   "metadata": {},
   "source": [
    "Not surprisingly, Harrisonburg is the top toponym, but there are also some \"garbage\" locations like `FCS` and `Breeze` that are throwing off the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c2e93-1c73-4c30-8a66-13cb9342b47e",
   "metadata": {},
   "source": [
    "### 6.1  Visualizing Toponyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67363cf5-9c7a-4a04-8d23-ed46cbfc3526",
   "metadata": {},
   "source": [
    "It is much easier to look at this data in a chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a09c3bd-53f4-40fc-a155-c6755a94c744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "Toponym=%{x}<br>Frequency=%{text}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "text": {
          "bdata": "AAAAAAAAbkAAAAAAAIBcQAAAAAAAgEdAAAAAAAAARUAAAAAAAIBEQAAAAAAAADVAAAAAAAAANEAAAAAAAAAzQAAAAAAAADNAAAAAAAAAM0A=",
          "dtype": "f8"
         },
         "textposition": "auto",
         "type": "bar",
         "x": [
          "Harrisonburg",
          "Virginia",
          "VT",
          "VA",
          "US",
          "Florida",
          "harrisonburg",
          "America",
          "Breeze",
          "FCS"
         ],
         "xaxis": "x",
         "y": {
          "bdata": "8AByAC8AKgApABUAFAATABMAEwA=",
          "dtype": "i2"
         },
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Top 10 Most Common Toponyms"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Toponym"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Frequency"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Take the top 10 most common toponyms for plotting\n",
    "toponym_counts_top10 = toponym_counts_df.head(10)\n",
    "\n",
    "# Create the bar chart using Plotly\n",
    "fig = px.bar(\n",
    "    toponym_counts_top10,\n",
    "    x='Toponym',\n",
    "    y='Count',\n",
    "    title='Top 10 Most Common Toponyms',\n",
    "    labels={'Toponym': 'Toponym', 'Count': 'Frequency'},\n",
    "    text='Count'\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7348468",
   "metadata": {},
   "source": [
    "### Save Progress\n",
    "\n",
    "Now that we have run the tokenizer we will want to save the newly created data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb9efb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df_reddit_sentences, 'data/jmu_reddit_toponyms.pickle')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds101_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
